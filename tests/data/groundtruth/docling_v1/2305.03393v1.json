{"_name": "", "type": "pdf-document", "description": {"title": null, "abstract": null, "authors": null, "affiliations": null, "subjects": null, "keywords": null, "publication_date": null, "languages": null, "license": null, "publishers": null, "url_refs": null, "references": null, "publication": null, "reference_count": null, "citation_count": null, "citation_date": null, "advanced": null, "analytics": null, "logs": [], "collection": null, "acquisition": null}, "file-info": {"filename": "2305.03393v1.pdf", "filename-prov": null, "document-hash": "c98927fda1ef9b66a4c3a236a65dc0cdf5c129be4122cdb58eaa3a37e3241eae", "#-pages": 14, "collection-name": null, "description": null, "page-hashes": [{"hash": "f09df98501fbcd8a2b359e4686187b56b7d82f3eb312cbbb23f61661691ecbf9", "model": "default", "page": 1}, {"hash": "6d26558563949e376cdb8dcb12a7288ec12d4c513de04616238aadcd15255d28", "model": "default", "page": 2}, {"hash": "4ef8043e938e362a06bc7f88f0b02df95d95cbfc891f544b7f88a448e53fb689", "model": "default", "page": 3}, {"hash": "8b755c3cd938ebf88bf14db6103c999794b0ca0c6f591f47a0c902b111159fe6", "model": "default", "page": 4}, {"hash": "95582f3138775a800969e873ad2e4eafca4f1d1de7b9b14ad826bbe8a17fe302", "model": "default", "page": 5}, {"hash": "619ab9fe3258434818f86df106cb76ed1fc8ab9800cbd91444098e91f7e67d8b", "model": "default", "page": 6}, {"hash": "c02e90eed528fcb71d0657183903b3e2035b86e3e750fb579f8c1f1e09aa132d", "model": "default", "page": 7}, {"hash": "b56262de55611de4494b0ed5011ce9567fada7c99bf53c5ff6c689ad9f941730", "model": "default", "page": 8}, {"hash": "680962e4a1193f15a591c82e1be59c0ff4cc78a066aeaaccad41f9262c67197b", "model": "default", "page": 9}, {"hash": "37dca86674661a5845a3bbd2fabb4a497cf2b5fc4908fd28dd63296c4fbee075", "model": "default", "page": 10}, {"hash": "0e3c057d1d7e6b359d73d4a44597879b2d421097da9aeb18ea581b32666ce740", "model": "default", "page": 11}, {"hash": "ec343c5522af29f238bde237ca655cdc018c5db20fb099c15ce8bc5045ce8593", "model": "default", "page": 12}, {"hash": "4ffa1d69b1366de506ca77c25a021790c3c150791fc830d6f4c85c3846efe6a9", "model": "default", "page": 13}, {"hash": "9fd62e0449eaf680e49767b4c512d8172cd3586480344318dc7e1cb0964b4d18", "model": "default", "page": 14}]}, "main-text": [{"prov": [{"bbox": [134.765, 645.486, 480.597, 676.101], "page": 1, "span": [0, 60], "__ref_s3_data": null}], "text": "Optimized Table Tokenization for Table Structure Recognition", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [139.343, 591.836, 476.013, 621.841], "page": 1, "span": [0, 208], "__ref_s3_data": null}], "text": "Maksym Lysak [0000 - 0002 - 3723 - 6960] , Ahmed Nassar [0000 - 0002 - 9468 - 0822] , Nikolaos Livathinos [0000 - 0001 - 8513 - 3491] , Christoph Auer [0000 - 0001 - 5761 - 0422] , [0000 - 0002 - 8088 - 0823]", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [229.521, 587.619, 298.609, 596.416], "page": 1, "span": [0, 15], "__ref_s3_data": null}], "text": "and Peter Staar", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [279.105, 565.785, 336.251, 577.074], "page": 1, "span": [0, 12], "__ref_s3_data": null}], "text": "IBM Research", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [222.966, 555.722, 392.39, 563.191], "page": 1, "span": [0, 36], "__ref_s3_data": null}], "text": "{mly,ahn,nli,cau,taa}@zurich.ibm.com", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [163.111, 326.324, 452.249, 523.914], "page": 1, "span": [0, 1198], "__ref_s3_data": null}], "text": "Abstract. Extracting tables from documents is a crucial task in any document conversion pipeline. Recently, transformer-based models have demonstrated that table-structure can be recognized with impressive accuracy using Image-to-Markup-Sequence (Im2Seq) approaches. Taking only the image of a table, such models predict a sequence of tokens (e.g. in HTML, LaTeX) which represent the structure of the table. Since the token representation of the table structure has a significant impact on the accuracy and run-time performance of any Im2Seq model, we investigate in this paper how table-structure representation can be optimised. We propose a new, optimised table-structure language (OTSL) with a minimized vocabulary and specific rules. The benefits of OTSL are that it reduces the number of tokens to 5 (HTML needs 28+) and shortens the sequence length to half of HTML on average. Consequently, model accuracy improves significantly, inference time is halved compared to HTML-based models, and the predicted table structures are always syntactically correct. This in turn eliminates most post-processing needs. Popular table structure data-sets will be published in OTSL format to the community.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [163.111, 293.273, 452.241, 315.521], "page": 1, "span": [0, 90], "__ref_s3_data": null}], "text": "Keywords: Table Structure Recognition \u00b7 Data Representation \u00b7 Transformers \u00b7 Optimization.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 259.312, 228.934, 269.88], "page": 1, "span": [0, 14], "__ref_s3_data": null}], "text": "1 Introduction", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [134.765, 163.18499999999995, 480.596, 243.71299999999997], "page": 1, "span": [0, 500], "__ref_s3_data": null}], "text": "Tables are ubiquitous in documents such as scientific papers, patents, reports, manuals, specification sheets or marketing material. They often encode highly valuable information and therefore need to be extracted with high accuracy. Unfortunately, tables appear in documents in various sizes, styling and structure, making it difficult to recover their correct structure with simple analytical methods. Therefore, accurate table extraction is achieved these days with machine-learning based methods.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 127.14499999999998, 480.596, 159.85199999999998], "page": 1, "span": [0, 235], "__ref_s3_data": null}], "text": "In modern document understanding systems [1,15], table extraction is typically a two-step process. Firstly, every table on a page is located with a bounding box, and secondly, their logical row and column structure is recognized. As of", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"name": "Picture", "type": "figure", "$ref": "#/figures/0"}, {"prov": [{"bbox": [134.765, 271.11300000000006, 480.592, 339.686], "page": 2, "span": [0, 435], "__ref_s3_data": null}], "text": "today, table detection in documents is a well understood problem, and the latest state-of-the-art (SOTA) object detection methods provide an accuracy comparable to human observers [7,8,10,14,23]. On the other hand, the problem of table structure recognition (TSR) is a lot more challenging and remains a very active area of research, in which many novel machine learning algorithms are being explored [3,4,5,9,11,12,13,14,17,18,21,22].", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 127.14499999999998, 480.595, 267.44900000000007], "page": 2, "span": [0, 911], "__ref_s3_data": null}], "text": "Recently emerging SOTA methods for table structure recognition employ transformer-based models, in which an image of the table is provided to the network in order to predict the structure of the table as a sequence of tokens. These image-to-sequence (Im2Seq) models are extremely powerful, since they allow for a purely data-driven solution. The tokens of the sequence typically belong to a markup language such as HTML, Latex or Markdown, which allow to describe table structure as rows, columns and spanning cells in various configurations. In Figure 1, we illustrate how HTML is used to represent the table-structure of a particular example table. Public table-structure data sets such as PubTabNet [22], and FinTabNet [21], which were created in a semi-automated way from paired PDF and HTML sources (e.g. PubMed Central), popularized primarily the use of HTML as ground-truth representation format for TSR.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 580.583, 480.592, 673.066], "page": 3, "span": [0, 584], "__ref_s3_data": null}], "text": "While the majority of research in TSR is currently focused on the development and application of novel neural model architectures, the table structure representation language (e.g. HTML in PubTabNet and FinTabNet) is usually adopted as is for the sequence tokenization in Im2Seq models. In this paper, we aim for the opposite and investigate the impact of the table structure representation language with an otherwise unmodified Im2Seq transformer-based architecture. Since the current state-of-the-art Im2Seq model is TableFormer [9], we select this model to perform our experiments.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 460.77, 480.596, 577.164], "page": 3, "span": [0, 721], "__ref_s3_data": null}], "text": "The main contribution of this paper is the introduction of a new optimised table structure language (OTSL), specifically designed to describe table-structure in an compact and structured way for Im2Seq models. OTSL has a number of key features, which make it very attractive to use in Im2Seq models. Specifically, compared to other languages such as HTML, OTSL has a minimized vocabulary which yields short sequence length, strong inherent structure (e.g. strict rectangular layout) and a strict syntax with rules that only look backwards. The latter allows for syntax validation during inference and ensures a syntactically correct table-structure. These OTSL features are illustrated in Figure 1, in comparison to HTML.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 352.913, 480.596, 457.352], "page": 3, "span": [0, 626], "__ref_s3_data": null}], "text": "The paper is structured as follows. In section 2, we give an overview of the latest developments in table-structure reconstruction. In section 3 we review the current HTML table encoding (popularised by PubTabNet and FinTabNet) and discuss its flaws. Subsequently, we introduce OTSL in section 4, which includes the language definition, syntax rules and error-correction procedures. In section 5, we apply OTSL on the TableFormer architecture, compare it to TableFormer models trained on HTML and ultimately demonstrate the advantages of using OTSL. Finally, in section 6 we conclude our work and outline next potential steps.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 319.344, 236.769, 329.912], "page": 3, "span": [0, 14], "__ref_s3_data": null}], "text": "2 Related Work", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [134.765, 127.144, 484.121, 303.314], "page": 3, "span": [0, 1161], "__ref_s3_data": null}], "text": "Approaches to formalize the logical structure and layout of tables in electronic documents date back more than two decades [16]. In the recent past, a wide variety of computer vision methods have been explored to tackle the problem of table structure recognition, i.e. the correct identification of columns, rows and spanning cells in a given table. Broadly speaking, the current deeplearning based approaches fall into three categories: object detection (OD) methods, Graph-Neural-Network (GNN) methods and Image-to-Markup-Sequence (Im2Seq) methods. Object-detection based methods [11,12,13,14,21] rely on tablestructure annotation using (overlapping) bounding boxes for training, and produce bounding-box predictions to define table cells, rows, and columns on a table image. Graph Neural Network (GNN) based methods [3,6,17,18], as the name suggests, represent tables as graph structures. The graph nodes represent the content of each table cell, an embedding vector from the table image, or geometric coordinates of the table cell. The edges of the graph define the relationship between the nodes, e.g. if they belong to the same column, row, or table cell.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 532.762, 480.596, 673.066], "page": 4, "span": [0, 939], "__ref_s3_data": null}], "text": "Other work [20] aims at predicting a grid for each table and deciding which cells must be merged using an attention network. Im2Seq methods cast the problem as a sequence generation task [4,5,9,22], and therefore need an internal tablestructure representation language, which is often implemented with standard markup languages (e.g. HTML, LaTeX, Markdown). In theory, Im2Seq methods have a natural advantage over the OD and GNN methods by virtue of directly predicting the table-structure. As such, no post-processing or rules are needed in order to obtain the table-structure, which is necessary with OD and GNN approaches. In practice, this is not entirely true, because a predicted sequence of table-structure markup does not necessarily have to be syntactically correct. Hence, depending on the quality of the predicted sequence, some post-processing needs to be performed to ensure a syntactically valid (let alone correct) sequence.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 305.353, 480.596, 529.343], "page": 4, "span": [0, 1404], "__ref_s3_data": null}], "text": "Within the Im2Seq method, we find several popular models, namely the encoder-dual-decoder model (EDD) [22], TableFormer [9], Tabsplitter[2] and Ye et. al. [19]. EDD uses two consecutive long short-term memory (LSTM) decoders to predict a table in HTML representation. The tag decoder predicts a sequence of HTML tags. For each decoded table cell ( <td> ), the attention is passed to the cell decoder to predict the content with an embedded OCR approach. The latter makes it susceptible to transcription errors in the cell content of the table. TableFormer address this reliance on OCR and uses two transformer decoders for HTML structure and cell bounding box prediction in an end-to-end architecture. The predicted cell bounding box is then used to extract text tokens from an originating (digital) PDF page, circumventing any need for OCR. TabSplitter [2] proposes a compact double-matrix representation of table rows and columns to do error detection and error correction of HTML structure sequences based on predictions from [19]. This compact double-matrix representation can not be used directly by the Img2seq model training, so the model uses HTML as an intermediate form. Chi et. al. [4] introduce a data set and a baseline method using bidirectional LSTMs to predict LaTeX code. Kayal [5] introduces Gated ResNet transformers to predict LaTeX code, and a separate OCR module to extract content.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 209.45100000000002, 480.594, 301.934], "page": 4, "span": [0, 572], "__ref_s3_data": null}], "text": "Im2Seq approaches have shown to be well-suited for the TSR task and allow a full end-to-end network design that can output the final table structure without pre- or post-processing logic. Furthermore, Im2Seq models have demonstrated to deliver state-of-the-art prediction accuracy [9]. This motivated the authors to investigate if the performance (both in accuracy and inference time) can be further improved by optimising the table structure representation language. We believe this is a necessary step before further improving neural network architectures for this task.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 175.88200000000006, 269.624, 186.45000000000005], "page": 4, "span": [0, 19], "__ref_s3_data": null}], "text": "3 Problem Statement", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [134.765, 127.144, 480.594, 159.85199999999998], "page": 4, "span": [0, 233], "__ref_s3_data": null}], "text": "All known Im2Seq based models for TSR fundamentally work in similar ways. Given an image of a table, the Im2Seq model predicts the structure of the table by generating a sequence of tokens. These tokens originate from a finite vocab-", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 604.493, 480.594, 673.066], "page": 5, "span": [0, 422], "__ref_s3_data": null}], "text": "ulary and can be interpreted as a table structure. For example, with the HTML tokens <table> , </table> , <tr> , </tr> , <td> and </td> , one can construct simple table structures without any spanning cells. In reality though, one needs at least 28 HTML tokens to describe the most common complex tables observed in real-world documents [21,22], due to a variety of spanning cells definitions in the HTML token vocabulary.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"name": "Picture", "type": "figure", "$ref": "#/figures/1"}, {"prov": [{"bbox": [134.765, 259.57899999999995, 480.595, 423.793], "page": 5, "span": [0, 1021], "__ref_s3_data": null}], "text": "Obviously, HTML and other general-purpose markup languages were not designed for Im2Seq models. As such, they have some serious drawbacks. First, the token vocabulary needs to be artificially large in order to describe all plausible tabular structures. Since most Im2Seq models use an autoregressive approach, they generate the sequence token by token. Therefore, to reduce inference time, a shorter sequence length is critical. Every table-cell is represented by at least two tokens ( <td> and </td> ). Furthermore, when tokenizing the HTML structure, one needs to explicitly enumerate possible column-spans and row-spans as words. In practice, this ends up requiring 28 different HTML tokens (when including column- and row-spans up to 10 cells) just to describe every table in the PubTabNet dataset. Clearly, not every token is equally represented, as is depicted in Figure 2. This skewed distribution of tokens in combination with variable token row-length makes it challenging for models to learn the HTML structure.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 211.29399999999998, 480.593, 255.957], "page": 5, "span": [0, 313], "__ref_s3_data": null}], "text": "Additionally, it would be desirable if the representation would easily allow an early detection of invalid sequences on-the-go, before the prediction of the entire table structure is completed. HTML is not well-suited for this purpose as the verification of incomplete sequences is non-trivial or even impossible.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 127.14499999999998, 480.595, 207.673], "page": 5, "span": [0, 542], "__ref_s3_data": null}], "text": "In a valid HTML table, the token sequence must describe a 2D grid of table cells, serialised in row-major ordering, where each row and each column have the same length (while considering row- and column-spans). Furthermore, every opening tag in HTML needs to be matched by a closing tag in a correct hierarchical manner. Since the number of tokens for each table row and column can vary significantly, especially for large tables with many row- and column-spans, it is complex to verify the consistency of predicted structures during sequence", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 652.314, 480.595, 673.066], "page": 6, "span": [0, 132], "__ref_s3_data": null}], "text": "generation. Implicitly, this also means that Im2Seq models need to learn these complex syntax rules, simply to deliver valid output.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 496.258, 480.596, 648.517], "page": 6, "span": [0, 977], "__ref_s3_data": null}], "text": "In practice, we observe two major issues with prediction quality when training Im2Seq models on HTML table structure generation from images. On the one hand, we find that on large tables, the visual attention of the model often starts to drift and is not accurately moving forward cell by cell anymore. This manifests itself in either in an increasing location drift for proposed table-cells in later rows on the same column or even complete loss of vertical alignment, as illustrated in Figure 5. Addressing this with post-processing is partially possible, but clearly undesired. On the other hand, we find many instances of predictions with structural inconsistencies or plain invalid HTML output, as shown in Figure 6, which are nearly impossible to properly correct. Both problems seriously impact the TSR model performance, since they reflect not only in the task of pure structure recognition but also in the equally crucial recognition or matching of table cell content.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 460.801, 372.508, 471.369], "page": 6, "span": [0, 36], "__ref_s3_data": null}], "text": "4 Optimised Table Structure Language", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [134.765, 350.4, 480.595, 442.883], "page": 6, "span": [0, 563], "__ref_s3_data": null}], "text": "To mitigate the issues with HTML in Im2Seq-based TSR models laid out before, we propose here our Optimised Table Structure Language (OTSL). OTSL is designed to express table structure with a minimized vocabulary and a simple set of rules, which are both significantly reduced compared to HTML. At the same time, OTSL enables easy error detection and correction during sequence generation. We further demonstrate how the compact structure representation and minimized sequence length improves prediction accuracy and inference time in the TableFormer architecture.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 317.321, 261.801, 326.128], "page": 6, "span": [0, 23], "__ref_s3_data": null}], "text": "4.1 Language Definition", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [134.765, 270.294, 480.589, 303.002], "page": 6, "span": [0, 165], "__ref_s3_data": null}], "text": "In Figure 3, we illustrate how the OTSL is defined. In essence, the OTSL defines only 5 tokens that directly describe a tabular structure based on an atomic 2D grid.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [149.709, 257.701, 409.311, 266.49800000000005], "page": 6, "span": [0, 57], "__ref_s3_data": null}], "text": "The OTSL vocabulary is comprised of the following tokens:", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [140.993, 235.22299999999996, 460.544, 244.02999999999997], "page": 6, "span": [0, 72], "__ref_s3_data": null}], "text": "- -\"C\" cell a new table cell that either has or does not have cell content", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [140.993, 210.67499999999995, 480.594, 231.437], "page": 6, "span": [0, 82], "__ref_s3_data": null}], "text": "- -\"L\" cell left-looking cell , merging with the left neighbor cell to create a span", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [140.993, 186.12599999999998, 480.588, 206.88800000000003], "page": 6, "span": [0, 81], "__ref_s3_data": null}], "text": "- -\"U\" cell up-looking cell , merging with the upper neighbor cell to create a span", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [140.993, 173.53300000000002, 454.555, 182.34000000000003], "page": 6, "span": [0, 71], "__ref_s3_data": null}], "text": "- -\"X\" cell cross cell , to merge with both left and upper neighbor cells", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [140.993, 160.93899999999996, 328.617, 169.74599999999998], "page": 6, "span": [0, 40], "__ref_s3_data": null}], "text": "- -\"NL\" new-line , switch to the next row.", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [134.765, 127.14499999999998, 480.593, 147.89699999999993], "page": 6, "span": [0, 99], "__ref_s3_data": null}], "text": "A notable attribute of OTSL is that it has the capability of achieving lossless conversion to HTML.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"name": "Picture", "type": "figure", "$ref": "#/figures/2"}, {"prov": [{"bbox": [134.765, 477.897, 246.652, 486.704], "page": 7, "span": [0, 19], "__ref_s3_data": null}], "text": "4.2 Language Syntax", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [134.765, 457.955, 363.796, 466.752], "page": 7, "span": [0, 51], "__ref_s3_data": null}], "text": "The OTSL representation follows these syntax rules:", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [138.973, 424.066, 480.589, 444.829], "page": 7, "span": [0, 108], "__ref_s3_data": null}], "text": "- 1. Left-looking cell rule : The left neighbour of an \"L\" cell must be either another \"L\" cell or a \"C\" cell.", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [138.973, 400.153, 480.592, 420.915], "page": 7, "span": [0, 106], "__ref_s3_data": null}], "text": "- 2. Up-looking cell rule : The upper neighbour of a \"U\" cell must be either another \"U\" cell or a \"C\" cell.", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [138.973, 388.195, 226.074, 397.002], "page": 7, "span": [0, 20], "__ref_s3_data": null}], "text": "3. Cross cell rule :", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [151.701, 352.326, 480.592, 385.033], "page": 7, "span": [0, 167], "__ref_s3_data": null}], "text": "- The left neighbour of an \"X\" cell must be either another \"X\" cell or a \"U\" cell, and the upper neighbour of an \"X\" cell must be either another \"X\" cell or an \"L\" cell.", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [138.973, 340.367, 474.59, 349.174], "page": 7, "span": [0, 78], "__ref_s3_data": null}], "text": "- 4. First row rule : Only \"L\" cells and \"C\" cells are allowed in the first row.", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [138.973, 316.454, 480.588, 337.216], "page": 7, "span": [0, 84], "__ref_s3_data": null}], "text": "- 5. First column rule : Only \"U\" cells and \"C\" cells are allowed in the first column.", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [138.973, 292.54, 480.595, 313.303], "page": 7, "span": [0, 144], "__ref_s3_data": null}], "text": "- 6. Rectangular rule : The table representation is always rectangular - all rows must have an equal number of tokens, terminated with \"NL\" token.", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [134.765, 151.058, 480.596, 279.4069999999999], "page": 7, "span": [0, 848], "__ref_s3_data": null}], "text": "The application of these rules gives OTSL a set of unique properties. First of all, the OTSL enforces a strictly rectangular structure representation, where every new-line token starts a new row. As a consequence, all rows and all columns have exactly the same number of tokens, irrespective of cell spans. Secondly, the OTSL representation is unambiguous: Every table structure is represented in one way. In this representation every table cell corresponds to a \"C\"-cell token, which in case of spans is always located in the top-left corner of the table cell definition. Third, OTSL syntax rules are only backward-looking. As a consequence, every predicted token can be validated straight during sequence generation by looking at the previously predicted sequence. As such, OTSL can guarantee that every predicted sequence is syntactically valid.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 127.14499999999998, 480.593, 147.89699999999993], "page": 7, "span": [0, 153], "__ref_s3_data": null}], "text": "These characteristics can be easily learned by sequence generator networks, as we demonstrate further below. We find strong indications that this pattern", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 652.314, 480.589, 673.066], "page": 8, "span": [0, 84], "__ref_s3_data": null}], "text": "reduces significantly the column drift seen in the HTML based models (see Figure 5).", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 621.636, 319.347, 630.443], "page": 8, "span": [0, 35], "__ref_s3_data": null}], "text": "4.3 Error-detection and -mitigation", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [134.765, 493.324, 480.596, 609.718], "page": 8, "span": [0, 797], "__ref_s3_data": null}], "text": "The design of OTSL allows to validate a table structure easily on an unfinished sequence. The detection of an invalid sequence token is a clear indication of a prediction mistake, however a valid sequence by itself does not guarantee prediction correctness. Different heuristics can be used to correct token errors in an invalid sequence and thus increase the chances for accurate predictions. Such heuristics can be applied either after the prediction of each token, or at the end on the entire predicted sequence. For example a simple heuristic which can correct the predicted OTSL sequence on-the-fly is to verify if the token with the highest prediction confidence invalidates the predicted sequence, and replace it by the token with the next highest confidence until OTSL rules are satisfied.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 460.268, 229.035, 470.836], "page": 8, "span": [0, 13], "__ref_s3_data": null}], "text": "5 Experiments", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [134.765, 340.312, 480.595, 444.75], "page": 8, "span": [0, 684], "__ref_s3_data": null}], "text": "To evaluate the impact of OTSL on prediction accuracy and inference times, we conducted a series of experiments based on the TableFormer model (Figure 4) with two objectives: Firstly we evaluate the prediction quality and performance of OTSL vs. HTML after performing Hyper Parameter Optimization (HPO) on the canonical PubTabNet data set. Secondly we pick the best hyper-parameters found in the first step and evaluate how OTSL impacts the performance of TableFormer after training on other publicly available data sets (FinTabNet, PubTables-1M [14]). The ground truth (GT) from all data sets has been converted into OTSL format for this purpose, and will be made publicly available.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"name": "Picture", "type": "figure", "$ref": "#/figures/3"}, {"prov": [{"bbox": [134.765, 127.14499999999998, 480.592, 171.80700000000002], "page": 8, "span": [0, 299], "__ref_s3_data": null}], "text": "We rely on standard metrics such as Tree Edit Distance score (TEDs) for table structure prediction, and Mean Average Precision (mAP) with 0.75 Intersection Over Union (IOU) threshold for the bounding-box predictions of table cells. The predicted OTSL structures were converted back to HTML format in", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 640.358, 480.596, 673.066], "page": 9, "span": [0, 163], "__ref_s3_data": null}], "text": "order to compute the TED score. Inference timing results for all experiments were obtained from the same machine on a single core with AMD EPYC 7763 CPU @2.45 GHz.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 614.007, 318.448, 622.814], "page": 9, "span": [0, 32], "__ref_s3_data": null}], "text": "5.1 Hyper Parameter Optimization", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [134.765, 537.841, 480.593, 606.414], "page": 9, "span": [0, 423], "__ref_s3_data": null}], "text": "We have chosen the PubTabNet data set to perform HPO, since it includes a highly diverse set of tables. Also we report TED scores separately for simple and complex tables (tables with cell spans). Results are presented in Table. 1. It is evident that with OTSL, our model achieves the same TED score and slightly better mAP scores in comparison to HTML. However OTSL yields a 2x speed up in the inference runtime over HTML.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"name": "Table", "type": "table", "$ref": "#/tables/0"}, {"prov": [{"bbox": [134.765, 464.018, 480.595, 519.143], "page": 9, "span": [0, 398], "__ref_s3_data": null}], "text": "Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart.", "type": "caption", "payload": null, "name": "Caption", "font": null}, {"prov": [{"bbox": [134.765, 275.04099999999994, 264.403, 283.848], "page": 9, "span": [0, 24], "__ref_s3_data": null}], "text": "5.2 Quantitative Results", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [134.765, 174.96500000000003, 480.596, 267.44900000000007], "page": 9, "span": [0, 555], "__ref_s3_data": null}], "text": "We picked the model parameter configuration that produced the best prediction quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained and evaluated it on three publicly available data sets: PubTabNet (395k samples), FinTabNet (113k samples) and PubTables-1M (about 1M samples). Performance results are presented in Table. 2. It is clearly evident that the model trained on OTSL outperforms HTML across the board, keeping high TEDs and mAP scores even on difficult financial tables (FinTabNet) that contain sparse and large tables.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 127.14499999999998, 480.596, 171.80700000000002], "page": 9, "span": [0, 289], "__ref_s3_data": null}], "text": "Additionally, the results show that OTSL has an advantage over HTML when applied on a bigger data set like PubTables-1M and achieves significantly improved scores. Finally, OTSL achieves faster inference due to fewer decoding steps which is a result of the reduced sequence representation.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"name": "Table", "type": "table", "$ref": "#/tables/1"}, {"prov": [{"bbox": [134.765, 645.172, 480.594, 678.379], "page": 10, "span": [0, 192], "__ref_s3_data": null}], "text": "Table 2. TSR and cell detection results compared between OTSL and HTML on the PubTabNet [22], FinTabNet [21] and PubTables-1M [14] data sets using TableFormer [9] (with enc=6, dec=6, heads=8).", "type": "caption", "payload": null, "name": "Caption", "font": null}, {"prov": [{"bbox": [134.765, 494.278, 257.087, 503.085], "page": 10, "span": [0, 23], "__ref_s3_data": null}], "text": "5.3 Qualitative Results", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [134.765, 425.522, 480.59, 482.139], "page": 10, "span": [0, 309], "__ref_s3_data": null}], "text": "To illustrate the qualitative differences between OTSL and HTML, Figure 5 demonstrates less overlap and more accurate bounding boxes with OTSL. In Figure 6, OTSL proves to be more effective in handling tables with longer token sequences, resulting in even more precise structure prediction and bounding boxes.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"name": "Picture", "type": "figure", "$ref": "#/figures/4"}, {"prov": [{"bbox": [227.915, 116.654, 230.1, 126.17399999999998], "page": 10, "span": [0, 1], "__ref_s3_data": null}], "text": "\u03bc", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [300.581, 98.57100000000003, 302.726, 108.37800000000004], "page": 10, "span": [0, 1], "__ref_s3_data": null}], "text": "\u2265", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"name": "Picture", "type": "figure", "$ref": "#/figures/5"}, {"prov": [{"bbox": [134.765, 663.883, 219.255, 674.451], "page": 12, "span": [0, 12], "__ref_s3_data": null}], "text": "6 Conclusion", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [134.765, 588.518, 480.596, 645.136], "page": 12, "span": [0, 330], "__ref_s3_data": null}], "text": "We demonstrated that representing tables in HTML for the task of table structure recognition with Im2Seq models is ill-suited and has serious limitations. Furthermore, we presented in this paper an Optimized Table Structure Language (OTSL) which, when compared to commonly used general purpose languages, has several key benefits.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 468.163, 480.595, 584.556], "page": 12, "span": [0, 724], "__ref_s3_data": null}], "text": "First and foremost, given the same network configuration, inference time for a table-structure prediction is about 2 times faster compared to the conventional HTML approach. This is primarily owed to the shorter sequence length of the OTSL representation. Additional performance benefits can be obtained with HPO (hyper parameter optimization). As we demonstrate in our experiments, models trained on OTSL can be significantly smaller, e.g. by reducing the number of encoder and decoder layers, while preserving comparatively good prediction quality. This can further improve inference performance, yielding 5-6 times faster inference speed in OTSL with prediction quality comparable to models trained on HTML (see Table 1).", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 323.897, 480.595, 464.201], "page": 12, "span": [0, 926], "__ref_s3_data": null}], "text": "Secondly, OTSL has more inherent structure and a significantly restricted vocabulary size. This allows autoregressive models to perform better in the TED metric, but especially with regards to prediction accuracy of the table-cell bounding boxes (see Table 2). As shown in Figure 5, we observe that the OTSL drastically reduces the drift for table cell bounding boxes at high row count and in sparse tables. This leads to more accurate predictions and a significant reduction in post-processing complexity, which is an undesired necessity in HTML-based Im2Seq models. Significant novelty lies in OTSL syntactical rules, which are few, simple and always backwards looking. Each new token can be validated only by analyzing the sequence of previous tokens, without requiring the entire sequence to detect mistakes. This in return allows to perform structural error detection and correction on-the-fly during sequence generation.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 287.611, 197.686, 298.179], "page": 12, "span": [0, 10], "__ref_s3_data": null}], "text": "References", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [139.371, 228.12799999999993, 480.592, 271.398], "page": 12, "span": [0, 270], "__ref_s3_data": null}], "text": "- 1. Auer, C., Dolfi, M., Carvalho, A., Ramis, C.B., Staar, P.W.J.: Delivering document conversion as a cloud service with high throughput and responsiveness. CoRR abs/2206.00785 (2022). https://doi.org/10.48550/arXiv.2206.00785 , https://doi.org/10.48550/arXiv.2206.00785", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [139.371, 182.59299999999996, 480.592, 226.75900000000001], "page": 12, "span": [0, 301], "__ref_s3_data": null}], "text": "- 2. Chen, B., Peng, D., Zhang, J., Ren, Y., Jin, L.: Complex table structure recognition in the wild using transformer and identity matrix-based augmentation. In: Porwal, U., Forn\u00e9s, A., Shafait, F. (eds.) Frontiers in Handwriting Recognition. pp. 545561. Springer International Publishing, Cham (2022)", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [139.371, 159.87099999999998, 480.587, 182.11900000000003], "page": 12, "span": [0, 140], "__ref_s3_data": null}], "text": "- 3. Chi, Z., Huang, H., Xu, H.D., Yu, H., Yin, W., Mao, X.L.: Complicated table structure recognition. arXiv preprint arXiv:1908.04729 (2019)", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [139.371, 126.19100000000003, 480.588, 159.39699999999993], "page": 12, "span": [0, 204], "__ref_s3_data": null}], "text": "- 4. Deng, Y., Rosenberg, D., Mann, G.: Challenges in end-to-end neural scientific table recognition. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 894-901. IEEE (2019)", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [139.371, 641.397, 480.595, 674.604], "page": 13, "span": [0, 203], "__ref_s3_data": null}], "text": "- 5. Kayal, P., Anand, M., Desai, H., Singh, M.: Tables to latex: structure and content extraction from scientific tables. International Journal on Document Analysis and Recognition (IJDAR) pp. 1-10 (2022)", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [139.371, 575.621, 480.59, 597.869], "page": 13, "span": [0, 131], "__ref_s3_data": null}], "text": "- 7. Li, M., Cui, L., Huang, S., Wei, F., Zhou, M., Li, Z.: Tablebank: A benchmark dataset for table detection and recognition (2019)", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [139.371, 597.55, 480.593, 641.716], "page": 13, "span": [0, 264], "__ref_s3_data": null}], "text": "- 6. Lee, E., Kwon, J., Yang, H., Park, J., Lee, S., Koo, H.I., Cho, N.I.: Table structure recognition based on grid shape graph. In: 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). pp. 18681873. IEEE (2022)", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [139.371, 521.711, 480.595, 575.939], "page": 13, "span": [0, 345], "__ref_s3_data": null}], "text": "- 8. Livathinos, N., Berrospi, C., Lysak, M., Kuropiatnyk, V., Nassar, A., Carvalho, A., Dolfi, M., Auer, C., Dinkla, K., Staar, P.: Robust pdf document conversion using recurrent neural networks. Proceedings of the AAAI Conference on Artificial Intelligence 35 (17), 15137-15145 (May 2021), https://ojs.aaai.org/index.php/ AAAI/article/view/17777", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [139.371, 487.926, 480.594, 521.133], "page": 13, "span": [0, 234], "__ref_s3_data": null}], "text": "- 9. Nassar, A., Livathinos, N., Lysak, M., Staar, P.: Tableformer: Table structure understanding with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4614-4623 (June 2022)", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [134.764, 423.057, 480.594, 488.245], "page": 13, "span": [0, 413], "__ref_s3_data": null}], "text": "- 10. Pfitzmann, B., Auer, C., Dolfi, M., Nassar, A.S., Staar, P.W.J.: Doclaynet: A large human-annotated dataset for document-layout segmentation. In: Zhang, A., Rangwala, H. (eds.) KDD '22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022. pp. 3743-3751. ACM (2022). https://doi.org/10.1145/3534678.3539043 , https:// doi.org/10.1145/3534678.3539043", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [134.764, 378.314, 480.593, 422.48], "page": 13, "span": [0, 295], "__ref_s3_data": null}], "text": "- 11. Prasad, D., Gadpal, A., Kapadni, K., Visave, M., Sultanpure, K.: Cascadetabnet: An approach for end to end table detection and structure recognition from imagebased documents. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. pp. 572-573 (2020)", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [134.764, 291.516, 480.594, 334.786], "page": 13, "span": [0, 275], "__ref_s3_data": null}], "text": "- 13. Siddiqui, S.A., Fateh, I.A., Rizvi, S.T.R., Dengel, A., Ahmed, S.: Deeptabstr: Deep learning based table structure recognition. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 1403-1409 (2019). https:// doi.org/10.1109/ICDAR.2019.00226", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [134.764, 334.467, 480.595, 378.633], "page": 13, "span": [0, 281], "__ref_s3_data": null}], "text": "- 12. Schreiber, S., Agne, S., Wolf, I., Dengel, A., Ahmed, S.: Deepdesrt: Deep learning for detection and structure recognition of tables in document images. In: 2017 14th IAPR international conference on document analysis and recognition (ICDAR). vol. 1, pp. 1162-1167. IEEE (2017)", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [134.764, 246.77300000000002, 480.593, 290.939], "page": 13, "span": [0, 241], "__ref_s3_data": null}], "text": "- 14. Smock, B., Pesala, R., Abraham, R.: PubTables-1M: Towards comprehensive table extraction from unstructured documents. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4634-4642 (June 2022)", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [134.764, 181.904, 480.596, 247.09199999999998], "page": 13, "span": [0, 405], "__ref_s3_data": null}], "text": "- 15. Staar, P.W.J., Dolfi, M., Auer, C., Bekas, C.: Corpus conversion service: A machine learning platform to ingest documents at scale. In: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. pp. 774-782. KDD '18, Association for Computing Machinery, New York, NY, USA (2018). https://doi.org/10.1145/3219819.3219834 , https://doi.org/10. 1145/3219819.3219834", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [134.764, 159.07899999999995, 480.595, 181.327], "page": 13, "span": [0, 96], "__ref_s3_data": null}], "text": "- 16. Wang, X.: Tabular Abstraction, Editing, and Formatting. Ph.D. thesis, CAN (1996), aAINN09397", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [134.764, 126.19100000000003, 480.591, 159.39800000000002], "page": 13, "span": [0, 195], "__ref_s3_data": null}], "text": "- 17. Xue, W., Li, Q., Tao, D.: Res2tim: Reconstruct syntactic structures from table images. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 749-755. IEEE (2019)", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [134.765, 641.397, 480.591, 674.604], "page": 14, "span": [0, 223], "__ref_s3_data": null}], "text": "- 18. Xue, W., Yu, B., Wang, W., Tao, D., Li, Q.: Tgrnet: A table graph reconstruction network for table structure recognition. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1295-1304 (2021)", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [134.765, 598.457, 480.595, 641.727], "page": 14, "span": [0, 269], "__ref_s3_data": null}], "text": "- 19. Ye, J., Qi, X., He, Y., Chen, Y., Gu, D., Gao, P., Xiao, R.: Pingan-vcgroup's solution for icdar 2021 competition on scientific literature parsing task b: Table recognition to html (2021). https://doi.org/10.48550/ARXIV.2105.01848 , https://arxiv.org/abs/2105.01848", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [134.765, 575.644, 480.594, 597.891], "page": 14, "span": [0, 147], "__ref_s3_data": null}], "text": "- 20. Zhang, Z., Zhang, J., Du, J., Wang, F.: Split, embed and merge: An accurate table structure recognizer. Pattern Recognition 126 , 108565 (2022)", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [134.765, 521.745, 480.593, 575.974], "page": 14, "span": [0, 329], "__ref_s3_data": null}], "text": "- 21. Zheng, X., Burdick, D., Popa, L., Zhong, X., Wang, N.X.R.: Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context. In: 2021 IEEE Winter Conference on Applications of Computer Vision (WACV). pp. 697-706 (2021). https://doi.org/10.1109/WACV48630.2021. 00074", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [134.765, 477.013, 480.596, 521.179], "page": 14, "span": [0, 259], "__ref_s3_data": null}], "text": "- 22. Zhong, X., ShafieiBavani, E., Jimeno Yepes, A.: Image-based table recognition: Data, model, and evaluation. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M. (eds.) Computer Vision - ECCV 2020. pp. 564-580. Springer International Publishing, Cham (2020)", "type": "paragraph", "payload": null, "name": "List-item", "font": null}, {"prov": [{"bbox": [134.765, 444.137, 480.595, 477.343], "page": 14, "span": [0, 206], "__ref_s3_data": null}], "text": "- 23. Zhong, X., Tang, J., Yepes, A.J.: Publaynet: largest dataset ever for document layout analysis. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 1015-1022. IEEE (2019)", "type": "paragraph", "payload": null, "name": "List-item", "font": null}], "figures": [{"prov": [{"bbox": [148.45364379882812, 366.1537780761719, 464.3608093261719, 583.6257629394531], "page": 2, "span": [0, 574], "__ref_s3_data": null}], "text": "Fig. 1. Comparison between HTML and OTSL table structure representation: (A) table-example with complex row and column headers, including a 2D empty span, (B) minimal graphical representation of table structure using rectangular layout, (C) HTML representation, (D) OTSL representation. This example demonstrates many of the key-features of OTSL, namely its reduced vocabulary size (12 versus 5 in this case), its reduced sequence length (55 versus 30) and a enhanced internal structure (variable token sequence length per row in HTML versus a fixed length of rows in OTSL).", "type": "figure", "payload": null, "bounding-box": null}, {"prov": [{"bbox": [137.41448974609375, 451.7695007324219, 476.5608215332031, 558.4876861572266], "page": 5, "span": [0, 73], "__ref_s3_data": null}], "text": "Fig. 2. Frequency of tokens in HTML and OTSL as they appear in PubTabNet.", "type": "figure", "payload": null, "bounding-box": null}, {"prov": [{"bbox": [164.65028381347656, 511.6590576171875, 449.5505676269531, 628.2029113769531], "page": 7, "span": [0, 207], "__ref_s3_data": null}], "text": "Fig. 3. OTSL description of table structure: A - table example; B - graphical representation of table structure; C - mapping structure on a grid; D - OTSL structure encoding; E - explanation on cell encoding", "type": "figure", "payload": null, "bounding-box": null}, {"prov": [{"bbox": [140.70968627929688, 198.32281494140625, 472.73382568359375, 283.9361572265625], "page": 8, "span": [0, 104], "__ref_s3_data": null}], "text": "Fig. 4. Architecture sketch of the TableFormer model, which is a representative for the Im2Seq approach.", "type": "figure", "payload": null, "bounding-box": null}, {"prov": [{"bbox": [162.67430114746094, 128.78643798828125, 451.70062255859375, 347.37744140625], "page": 10, "span": [0, 270], "__ref_s3_data": null}], "text": "Fig. 5. The OTSL model produces more accurate bounding boxes with less overlap (E) than the HTML model (D), when predicting the structure of a sparse table (A), at twice the inference speed because of shorter sequence length (B),(C). \"PMC2807444_006_00.png\" PubTabNet. \u03bc", "type": "figure", "payload": null, "bounding-box": null}, {"prov": [{"bbox": [168.39285278320312, 157.99432373046875, 447.35137939453125, 610.0334930419922], "page": 11, "span": [0, 390], "__ref_s3_data": null}], "text": "Fig. 6. Visualization of predicted structure and detected bounding boxes on a complex table with many rows. The OTSL model (B) captured repeating pattern of horizontally merged cells from the GT (A), unlike the HTML model (C). The HTML model also didn't complete the HTML sequence correctly and displayed a lot more of drift and overlap of bounding boxes. \"PMC5406406_003_01.png\" PubTabNet.", "type": "figure", "payload": null, "bounding-box": null}], "tables": [{"prov": [{"bbox": [139.66845703125, 322.5278625488281, 475.00372314453125, 454.4252014160156], "page": 9, "span": [0, 0], "__ref_s3_data": null}], "text": "Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart.", "type": "table", "payload": null, "#-cols": 8, "#-rows": 6, "data": [[{"bbox": [144.592, 339.457, 183.828, 363.697], "spans": [[0, 0], [1, 0]], "text": "# enc-layers", "type": "col_header", "col": 0, "col-header": true, "col-span": [0, 1], "row": 0, "row-header": false, "row-span": [0, 2]}, {"bbox": [192.195, 339.457, 231.431, 363.697], "spans": [[0, 1], [1, 1]], "text": "# dec-layers", "type": "col_header", "col": 1, "col-header": true, "col-span": [1, 2], "row": 0, "row-header": false, "row-span": [0, 2]}, {"bbox": [239.798, 344.936, 278.318, 356.225], "spans": [[0, 2], [1, 2]], "text": "Language", "type": "col_header", "col": 2, "col-header": true, "col-span": [2, 3], "row": 0, "row-header": false, "row-span": [0, 2]}, {"bbox": [324.67, 339.457, 348.264, 350.746], "spans": [[0, 3], [0, 4], [0, 5]], "text": "TEDs", "type": "col_header", "col": 3, "col-header": true, "col-span": [3, 6], "row": 0, "row-header": false, "row-span": [0, 1]}, {"bbox": [324.67, 339.457, 348.264, 350.746], "spans": [[0, 3], [0, 4], [0, 5]], "text": "TEDs", "type": "col_header", "col": 4, "col-header": true, "col-span": [3, 6], "row": 0, "row-header": false, "row-span": [0, 1]}, {"bbox": [324.67, 339.457, 348.264, 350.746], "spans": [[0, 3], [0, 4], [0, 5]], "text": "TEDs", "type": "col_header", "col": 5, "col-header": true, "col-span": [3, 6], "row": 0, "row-header": false, "row-span": [0, 1]}, {"bbox": [396.271, 339.457, 417.127, 350.746], "spans": [[0, 6]], "text": "mAP", "type": "col_header", "col": 6, "col-header": true, "col-span": [6, 7], "row": 0, "row-header": false, "row-span": [0, 1]}, {"bbox": [430.771, 339.457, 467.142, 350.746], "spans": [[0, 7]], "text": "Inference", "type": "col_header", "col": 7, "col-header": true, "col-span": [7, 8], "row": 0, "row-header": false, "row-span": [0, 1]}], [{"bbox": [144.592, 339.457, 183.828, 363.697], "spans": [[0, 0], [1, 0]], "text": "# enc-layers", "type": "col_header", "col": 0, "col-header": true, "col-span": [0, 1], "row": 1, "row-header": false, "row-span": [0, 2]}, {"bbox": [192.195, 339.457, 231.431, 363.697], "spans": [[0, 1], [1, 1]], "text": "# dec-layers", "type": "col_header", "col": 1, "col-header": true, "col-span": [1, 2], "row": 1, "row-header": false, "row-span": [0, 2]}, {"bbox": [239.798, 344.936, 278.318, 356.225], "spans": [[0, 2], [1, 2]], "text": "Language", "type": "col_header", "col": 2, "col-header": true, "col-span": [2, 3], "row": 1, "row-header": false, "row-span": [0, 2]}, {"bbox": [286.686, 352.408, 312.333, 363.697], "spans": [[1, 3]], "text": "simple", "type": "col_header", "col": 3, "col-header": true, "col-span": [3, 4], "row": 1, "row-header": false, "row-span": [1, 2]}, {"bbox": [320.702, 352.408, 353.72, 363.697], "spans": [[1, 4]], "text": "complex", "type": "col_header", "col": 4, "col-header": true, "col-span": [4, 5], "row": 1, "row-header": false, "row-span": [1, 2]}, {"bbox": [369.306, 352.408, 379.031, 363.697], "spans": [[1, 5]], "text": "all", "type": "col_header", "col": 5, "col-header": true, "col-span": [5, 6], "row": 1, "row-header": false, "row-span": [1, 2]}, {"bbox": [394.927, 350.416, 418.473, 361.705], "spans": [[1, 6]], "text": "(0.75)", "type": "col_header", "col": 6, "col-header": true, "col-span": [6, 7], "row": 1, "row-header": false, "row-span": [1, 2]}, {"bbox": [427.148, 350.416, 470.761, 361.705], "spans": [[1, 7]], "text": "time (secs)", "type": "col_header", "col": 7, "col-header": true, "col-span": [7, 8], "row": 1, "row-header": false, "row-span": [1, 2]}], [{"bbox": [161.906, 371.238, 166.513, 382.527], "spans": [[2, 0]], "text": "6", "type": "body", "col": 0, "col-header": false, "col-span": [0, 1], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [209.509, 371.238, 214.116, 382.527], "spans": [[2, 1]], "text": "6", "type": "body", "col": 1, "col-header": false, "col-span": [1, 2], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [245.176, 365.758, 272.94, 389.999], "spans": [[2, 2]], "text": "OTSL HTML", "type": "body", "col": 2, "col-header": false, "col-span": [2, 3], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [289.017, 365.758, 310.004, 389.999], "spans": [[2, 3]], "text": "0.965 0.969", "type": "body", "col": 3, "col-header": false, "col-span": [3, 4], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [326.717, 365.758, 347.704, 389.999], "spans": [[2, 4]], "text": "0.934 0.927", "type": "body", "col": 4, "col-header": false, "col-span": [4, 5], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [363.676, 365.758, 384.663, 389.999], "spans": [[2, 5]], "text": "0.955 0.955", "type": "body", "col": 5, "col-header": false, "col-span": [5, 6], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [396.206, 367.973, 417.193, 389.999], "spans": [[2, 6]], "text": "0.88 0.857", "type": "body", "col": 6, "col-header": false, "col-span": [6, 7], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [439.527, 367.973, 458.384, 389.999], "spans": [[2, 7]], "text": "2.73 5.39", "type": "body", "col": 7, "col-header": false, "col-span": [7, 8], "row": 2, "row-header": false, "row-span": [2, 3]}], [{"bbox": [161.906, 397.539, 166.513, 408.828], "spans": [[3, 0]], "text": "4", "type": "body", "col": 0, "col-header": false, "col-span": [0, 1], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [209.509, 397.539, 214.116, 408.828], "spans": [[3, 1]], "text": "4", "type": "body", "col": 1, "col-header": false, "col-span": [1, 2], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [245.176, 392.06, 272.94, 416.3], "spans": [[3, 2]], "text": "OTSL HTML", "type": "body", "col": 2, "col-header": false, "col-span": [2, 3], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [289.017, 392.06, 310.004, 416.3], "spans": [[3, 3]], "text": "0.938 0.952", "type": "body", "col": 3, "col-header": false, "col-span": [3, 4], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [326.717, 392.06, 347.704, 416.3], "spans": [[3, 4]], "text": "0.904 0.909", "type": "body", "col": 4, "col-header": false, "col-span": [4, 5], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [362.088, 392.06, 386.249, 415.152], "spans": [[3, 5]], "text": "0.927 0.938", "type": "body", "col": 5, "col-header": false, "col-span": [5, 6], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [394.618, 394.275, 418.779, 416.3], "spans": [[3, 6]], "text": "0.853 0.843", "type": "body", "col": 6, "col-header": false, "col-span": [6, 7], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [439.527, 394.275, 458.384, 416.3], "spans": [[3, 7]], "text": "1.97 3.77", "type": "body", "col": 7, "col-header": false, "col-span": [7, 8], "row": 3, "row-header": false, "row-span": [3, 4]}], [{"bbox": [161.906, 423.84, 166.513, 435.129], "spans": [[4, 0]], "text": "2", "type": "body", "col": 0, "col-header": false, "col-span": [0, 1], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [209.509, 423.84, 214.116, 435.129], "spans": [[4, 1]], "text": "4", "type": "body", "col": 1, "col-header": false, "col-span": [1, 2], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [245.176, 418.361, 272.94, 442.601], "spans": [[4, 2]], "text": "OTSL HTML", "type": "body", "col": 2, "col-header": false, "col-span": [2, 3], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [289.017, 418.361, 310.004, 442.601], "spans": [[4, 3]], "text": "0.923 0.945", "type": "body", "col": 3, "col-header": false, "col-span": [3, 4], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [326.717, 418.361, 347.704, 442.601], "spans": [[4, 4]], "text": "0.897 0.901", "type": "body", "col": 4, "col-header": false, "col-span": [4, 5], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [362.088, 418.361, 386.249, 441.453], "spans": [[4, 5]], "text": "0.915 0.931", "type": "body", "col": 5, "col-header": false, "col-span": [5, 6], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [394.618, 420.576, 418.779, 442.601], "spans": [[4, 6]], "text": "0.859 0.834", "type": "body", "col": 6, "col-header": false, "col-span": [6, 7], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [439.527, 420.576, 458.384, 442.601], "spans": [[4, 7]], "text": "1.91 3.81", "type": "body", "col": 7, "col-header": false, "col-span": [7, 8], "row": 4, "row-header": false, "row-span": [4, 5]}], [{"bbox": [161.906, 450.142, 166.513, 461.431], "spans": [[5, 0]], "text": "4", "type": "body", "col": 0, "col-header": false, "col-span": [0, 1], "row": 5, "row-header": false, "row-span": [5, 6]}, {"bbox": [209.509, 450.142, 214.116, 461.431], "spans": [[5, 1]], "text": "2", "type": "body", "col": 1, "col-header": false, "col-span": [1, 2], "row": 5, "row-header": false, "row-span": [5, 6]}, {"bbox": [245.176, 444.662, 272.94, 468.903], "spans": [[5, 2]], "text": "OTSL HTML", "type": "body", "col": 2, "col-header": false, "col-span": [2, 3], "row": 5, "row-header": false, "row-span": [5, 6]}, {"bbox": [289.017, 444.662, 310.004, 468.903], "spans": [[5, 3]], "text": "0.952 0.944", "type": "body", "col": 3, "col-header": false, "col-span": [3, 4], "row": 5, "row-header": false, "row-span": [5, 6]}, {"bbox": [326.717, 444.662, 347.704, 468.903], "spans": [[5, 4]], "text": "0.92 0.903", "type": "body", "col": 4, "col-header": false, "col-span": [4, 5], "row": 5, "row-header": false, "row-span": [5, 6]}, {"bbox": [362.088, 446.877, 386.249, 468.903], "spans": [[5, 5]], "text": "0.942 0.931", "type": "body", "col": 5, "col-header": false, "col-span": [5, 6], "row": 5, "row-header": false, "row-span": [5, 6]}, {"bbox": [394.618, 446.877, 418.779, 468.903], "spans": [[5, 6]], "text": "0.857 0.824", "type": "body", "col": 6, "col-header": false, "col-span": [6, 7], "row": 5, "row-header": false, "row-span": [5, 6]}, {"bbox": [439.527, 446.877, 458.384, 468.903], "spans": [[5, 7]], "text": "1.22 2", "type": "body", "col": 7, "col-header": false, "col-span": [7, 8], "row": 5, "row-header": false, "row-span": [5, 6]}]], "model": null, "bounding-box": null}, {"prov": [{"bbox": [143.6376495361328, 528.7375183105469, 470.8485412597656, 635.6522979736328], "page": 10, "span": [0, 0], "__ref_s3_data": null}], "text": "Table 2. TSR and cell detection results compared between OTSL and HTML on the PubTabNet [22], FinTabNet [21] and PubTables-1M [14] data sets using TableFormer [9] (with enc=6, dec=6, heads=8).", "type": "table", "payload": null, "#-cols": 7, "#-rows": 5, "data": [[{"bbox": [160.782, 164.28099999999995, 194.998, 175.57000000000005], "spans": [[0, 0], [1, 0]], "text": "Data set", "type": "col_header", "col": 0, "col-header": true, "col-span": [0, 1], "row": 0, "row-header": false, "row-span": [0, 2]}, {"bbox": [215.525, 164.25599999999997, 254.045, 175.54499999999996], "spans": [[0, 1], [1, 1]], "text": "Language", "type": "col_header", "col": 1, "col-header": true, "col-span": [1, 2], "row": 0, "row-header": false, "row-span": [0, 2]}, {"bbox": [300.397, 158.80100000000004, 323.991, 170.09000000000003], "spans": [[0, 2], [0, 3], [0, 4]], "text": "TEDs", "type": "col_header", "col": 2, "col-header": true, "col-span": [2, 5], "row": 0, "row-header": false, "row-span": [0, 1]}, {"bbox": [300.397, 158.80100000000004, 323.991, 170.09000000000003], "spans": [[0, 2], [0, 3], [0, 4]], "text": "TEDs", "type": "col_header", "col": 3, "col-header": true, "col-span": [2, 5], "row": 0, "row-header": false, "row-span": [0, 1]}, {"bbox": [300.397, 158.80100000000004, 323.991, 170.09000000000003], "spans": [[0, 2], [0, 3], [0, 4]], "text": "TEDs", "type": "col_header", "col": 4, "col-header": true, "col-span": [2, 5], "row": 0, "row-header": false, "row-span": [0, 1]}, {"bbox": [370.345, 164.28099999999995, 414.747, 175.57000000000005], "spans": [[0, 5], [1, 5]], "text": "mAP(0.75)", "type": "col_header", "col": 5, "col-header": true, "col-span": [5, 6], "row": 0, "row-header": false, "row-span": [0, 2]}, {"bbox": [423.114, 158.80100000000004, 466.727, 181.04899999999998], "spans": [[0, 6], [1, 6]], "text": "Inference time (secs)", "type": "col_header", "col": 6, "col-header": true, "col-span": [6, 7], "row": 0, "row-header": false, "row-span": [0, 2]}], [{"bbox": [160.782, 164.28099999999995, 194.998, 175.57000000000005], "spans": [[0, 0], [1, 0]], "text": "Data set", "type": "col_header", "col": 0, "col-header": true, "col-span": [0, 1], "row": 1, "row-header": false, "row-span": [0, 2]}, {"bbox": [215.525, 164.25599999999997, 254.045, 175.54499999999996], "spans": [[0, 1], [1, 1]], "text": "Language", "type": "col_header", "col": 1, "col-header": true, "col-span": [1, 2], "row": 1, "row-header": false, "row-span": [0, 2]}, {"bbox": [262.413, 171.75300000000004, 288.06, 183.04200000000003], "spans": [[1, 2]], "text": "simple", "type": "col_header", "col": 2, "col-header": true, "col-span": [2, 3], "row": 1, "row-header": false, "row-span": [1, 2]}, {"bbox": [296.429, 171.75300000000004, 329.447, 183.04200000000003], "spans": [[1, 3]], "text": "complex", "type": "col_header", "col": 3, "col-header": true, "col-span": [3, 4], "row": 1, "row-header": false, "row-span": [1, 2]}, {"bbox": [345.033, 171.75300000000004, 354.758, 183.04200000000003], "spans": [[1, 4]], "text": "all", "type": "col_header", "col": 4, "col-header": true, "col-span": [4, 5], "row": 1, "row-header": false, "row-span": [1, 2]}, {"bbox": [370.345, 164.28099999999995, 414.747, 175.57000000000005], "spans": [[0, 5], [1, 5]], "text": "mAP(0.75)", "type": "col_header", "col": 5, "col-header": true, "col-span": [5, 6], "row": 1, "row-header": false, "row-span": [0, 2]}, {"bbox": [423.114, 158.80100000000004, 466.727, 181.04899999999998], "spans": [[0, 6], [1, 6]], "text": "Inference time (secs)", "type": "col_header", "col": 6, "col-header": true, "col-span": [6, 7], "row": 1, "row-header": false, "row-span": [0, 2]}], [{"bbox": [154.538, 190.582, 201.241, 201.87099999999998], "spans": [[2, 0]], "text": "PubTabNet", "type": "body", "col": 0, "col-header": false, "col-span": [0, 1], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [220.903, 185.10299999999995, 248.667, 209.34299999999996], "spans": [[2, 1]], "text": "OTSL HTML", "type": "body", "col": 1, "col-header": false, "col-span": [1, 2], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [264.744, 185.10299999999995, 285.731, 209.34299999999996], "spans": [[2, 2]], "text": "0.965 0.969", "type": "body", "col": 2, "col-header": false, "col-span": [2, 3], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [302.444, 185.10299999999995, 323.431, 209.34299999999996], "spans": [[2, 3]], "text": "0.934 0.927", "type": "body", "col": 3, "col-header": false, "col-span": [3, 4], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [339.403, 185.10299999999995, 360.39, 209.34299999999996], "spans": [[2, 4]], "text": "0.955 0.955", "type": "body", "col": 4, "col-header": false, "col-span": [4, 5], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [382.052, 187.31799999999998, 403.039, 209.34299999999996], "spans": [[2, 5]], "text": "0.88 0.857", "type": "body", "col": 5, "col-header": false, "col-span": [5, 6], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [435.493, 187.31799999999998, 454.35, 209.34299999999996], "spans": [[2, 6]], "text": "2.73 5.39", "type": "body", "col": 6, "col-header": false, "col-span": [6, 7], "row": 2, "row-header": false, "row-span": [2, 3]}], [{"bbox": [155.945, 216.88400000000001, 199.834, 228.173], "spans": [[3, 0]], "text": "FinTabNet", "type": "body", "col": 0, "col-header": false, "col-span": [0, 1], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [220.903, 211.404, 248.667, 235.64499999999998], "spans": [[3, 1]], "text": "OTSL HTML", "type": "body", "col": 1, "col-header": false, "col-span": [1, 2], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [264.744, 211.404, 285.731, 235.64499999999998], "spans": [[3, 2]], "text": "0.955 0.917", "type": "body", "col": 2, "col-header": false, "col-span": [2, 3], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [302.444, 211.404, 323.431, 235.64499999999998], "spans": [[3, 3]], "text": "0.961 0.922", "type": "body", "col": 3, "col-header": false, "col-span": [3, 4], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [337.815, 213.61900000000003, 361.976, 235.64499999999998], "spans": [[3, 4]], "text": "0.959 0.92", "type": "body", "col": 4, "col-header": false, "col-span": [4, 5], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [380.464, 213.61900000000003, 404.625, 235.64499999999998], "spans": [[3, 5]], "text": "0.862 0.722", "type": "body", "col": 5, "col-header": false, "col-span": [5, 6], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [435.493, 213.61900000000003, 454.35, 235.64499999999998], "spans": [[3, 6]], "text": "1.85 3.26", "type": "body", "col": 6, "col-header": false, "col-span": [6, 7], "row": 3, "row-header": false, "row-span": [3, 4]}], [{"bbox": [148.626, 243.18499999999995, 207.152, 254.47400000000005], "spans": [[4, 0]], "text": "PubTables-1M", "type": "body", "col": 0, "col-header": false, "col-span": [0, 1], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [220.903, 237.70500000000004, 248.667, 261.946], "spans": [[4, 1]], "text": "OTSL HTML", "type": "body", "col": 1, "col-header": false, "col-span": [1, 2], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [264.744, 237.70500000000004, 285.731, 261.946], "spans": [[4, 2]], "text": "0.987 0.983", "type": "body", "col": 2, "col-header": false, "col-span": [2, 3], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [302.444, 237.70500000000004, 323.431, 261.946], "spans": [[4, 3]], "text": "0.964 0.944", "type": "body", "col": 3, "col-header": false, "col-span": [3, 4], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [337.815, 239.91999999999996, 361.976, 261.946], "spans": [[4, 4]], "text": "0.977 0.966", "type": "body", "col": 4, "col-header": false, "col-span": [4, 5], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [380.464, 239.91999999999996, 404.625, 261.946], "spans": [[4, 5]], "text": "0.896 0.889", "type": "body", "col": 5, "col-header": false, "col-span": [5, 6], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [435.493, 239.91999999999996, 454.35, 261.946], "spans": [[4, 6]], "text": "1.79 3.26", "type": "body", "col": 6, "col-header": false, "col-span": [6, 7], "row": 4, "row-header": false, "row-span": [4, 5]}]], "model": null, "bounding-box": null}], "bitmaps": null, "equations": [], "footnotes": [], "page-dimensions": [{"height": 792.0, "page": 1, "width": 612.0}, {"height": 792.0, "page": 2, "width": 612.0}, {"height": 792.0, "page": 3, "width": 612.0}, {"height": 792.0, "page": 4, "width": 612.0}, {"height": 792.0, "page": 5, "width": 612.0}, {"height": 792.0, "page": 6, "width": 612.0}, {"height": 792.0, "page": 7, "width": 612.0}, {"height": 792.0, "page": 8, "width": 612.0}, {"height": 792.0, "page": 9, "width": 612.0}, {"height": 792.0, "page": 10, "width": 612.0}, {"height": 792.0, "page": 11, "width": 612.0}, {"height": 792.0, "page": 12, "width": 612.0}, {"height": 792.0, "page": 13, "width": 612.0}, {"height": 792.0, "page": 14, "width": 612.0}], "page-footers": [], "page-headers": [], "_s3_data": null, "identifiers": null}