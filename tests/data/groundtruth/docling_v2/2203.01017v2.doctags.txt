<doctag><section_header_level_1><loc_79><loc_68><loc_408><loc_76>TableFormer: Table Structure Understanding with Transformers.</section_header_level_1>
<section_header_level_1><loc_116><loc_93><loc_370><loc_99>Ahmed Nassar, Nikolaos Livathinos, Maksym Lysak, Peter Staar</section_header_level_1>
<text><loc_170><loc_111><loc_174><loc_116>{</text>
<section_header_level_1><loc_119><loc_136><loc_156><loc_143>Abstract</section_header_level_1>
<text><loc_51><loc_152><loc_234><loc_158>Tables organize valuable content in a concise and com-</text>
<section_header_level_1><loc_41><loc_341><loc_104><loc_348>1. Introduction</section_header_level_1>
<text><loc_51><loc_354><loc_136><loc_360>The occurrence of tables</text>
<section_header_level_1><loc_258><loc_138><loc_334><loc_143>a. Picture of a table:</section_header_level_1>
<picture><loc_258><loc_144><loc_439><loc_191></picture>
<otsl><loc_258><loc_144><loc_439><loc_191></otsl>
<unordered_list><list_item><loc_258><loc_198><loc_397><loc_204>b. Red-annotation of bounding boxes,</list_item>
<list_item><loc_258><loc_265><loc_401><loc_271>c. Structure predicted by TableFormer:</list_item>
</unordered_list>
<picture><loc_257><loc_213><loc_441><loc_259></picture>
<picture><loc_258><loc_274><loc_439><loc_313><caption><loc_252><loc_325><loc_282><loc_330>Figure 1:</caption></picture>
<otsl><loc_258><loc_274><loc_439><loc_313></otsl>
<text><loc_262><loc_369><loc_293><loc_375>Recently,</text>
<text><loc_262><loc_422><loc_445><loc_428>The first problem is called table-location and has been</text>
<page_footer><loc_241><loc_464><loc_245><loc_469>1</page_footer>
<page_break>
<text><loc_41><loc_48><loc_234><loc_53>considered as a solved problem, given enough ground-truth</text>
<text><loc_51><loc_63><loc_234><loc_68>The second problem is called table-structure decompo-</text>
<text><loc_51><loc_146><loc_234><loc_152>In this paper, we want to address these weaknesses and</text>
<text><loc_51><loc_237><loc_234><loc_243>To meet the design criteria listed above, we developed a</text>
<unordered_list><list_item><loc_50><loc_281><loc_53><loc_286>•</list_item>
<list_item><loc_50><loc_317><loc_53><loc_323>•</list_item>
<list_item><loc_50><loc_354><loc_53><loc_359>•</list_item>
<list_item><loc_50><loc_382><loc_53><loc_388>•</list_item>
</unordered_list>
<text><loc_51><loc_411><loc_167><loc_416>The paper is structured as follows.</text>
<footnote><loc_50><loc_445><loc_52><loc_448>1</footnote>
<text><loc_252><loc_48><loc_445><loc_53>its results & performance in Sec. 5. As a conclusion, we de-</text>
<section_header_level_1><loc_252><loc_77><loc_407><loc_84>2. Previous work and State of the Art</section_header_level_1>
<text><loc_262><loc_90><loc_445><loc_95>Identifying the structure of a table has been an outstand-</text>
<text><loc_262><loc_211><loc_320><loc_216>Before the rising</text>
<text><loc_262><loc_286><loc_346><loc_292>Image-to-Text networks</text>
<page_footer><loc_241><loc_464><loc_245><loc_469>2</page_footer>
<page_break>
<text><loc_41><loc_48><loc_204><loc_53>tag-decoder which is constrained to the table-tags.</text>
<text><loc_51><loc_55><loc_57><loc_61>In</text>
<text><loc_51><loc_176><loc_74><loc_181>Graph</text>
<text><loc_51><loc_312><loc_207><loc_317>Hybrid Deep Learning-Rule-Based approach</text>
<section_header_level_1><loc_41><loc_401><loc_86><loc_408>3. Datasets</section_header_level_1>
<text><loc_51><loc_415><loc_234><loc_420>We rely on large-scale datasets such as PubTabNet [37],</text>
<picture><loc_255><loc_50><loc_450><loc_158><caption><loc_252><loc_169><loc_283><loc_175>Figure 2:</caption></picture>
<text><loc_252><loc_201><loc_357><loc_206>balance in the previous datasets.</text>
<text><loc_262><loc_209><loc_445><loc_215>The PubTabNet dataset contains 509k tables delivered as</text>
<text><loc_262><loc_399><loc_445><loc_405>Due to the heterogeneity across the dataset formats, it</text>
<page_footer><loc_241><loc_464><loc_245><loc_469>3</page_footer>
<page_break>
<text><loc_41><loc_48><loc_234><loc_53>amount of such tables, and kept only those ones ranging</text>
<text><loc_51><loc_64><loc_234><loc_69>The availability of the bounding boxes for all table cells</text>
<text><loc_51><loc_201><loc_234><loc_206>As it is illustrated in Fig. 2, the table distributions from</text>
<text><loc_51><loc_278><loc_234><loc_283>Motivated by those observations we aimed at generating</text>
<text><loc_51><loc_399><loc_234><loc_405>In this regard, we have prepared four synthetic datasets,</text>
<otsl><loc_254><loc_46><loc_444><loc_98><caption><loc_252><loc_106><loc_270><loc_111>Table</caption></otsl>
<text><loc_252><loc_158><loc_445><loc_163>one adopts a colorful appearance with high contrast and the</text>
<text><loc_262><loc_188><loc_443><loc_194>Tab. 1 summarizes the various attributes of the datasets.</text>
<section_header_level_1><loc_252><loc_203><loc_364><loc_210>4. The TableFormer model</section_header_level_1>
<text><loc_262><loc_216><loc_445><loc_221>Given the image of a table, TableFormer is able to pre-</text>
<section_header_level_1><loc_252><loc_289><loc_343><loc_295>4.1. Model architecture.</section_header_level_1>
<text><loc_262><loc_301><loc_445><loc_307>We now describe in detail the proposed method, which</text>
<text><loc_262><loc_422><loc_353><loc_428>CNN Backbone Network.</text>
<page_footer><loc_241><loc_464><loc_245><loc_469>4</page_footer>
<page_break>
<picture><loc_61><loc_49><loc_425><loc_116><caption><loc_41><loc_129><loc_71><loc_134>Figure 3:</caption></picture>
<picture><loc_43><loc_163><loc_233><loc_320><caption><loc_41><loc_333><loc_70><loc_339>Figure 4:</caption></picture>
<text><loc_252><loc_158><loc_445><loc_163>forming classification, and adding an adaptive pooling layer</text>
<text><loc_262><loc_188><loc_328><loc_193>Structure Decoder.</text>
<text><loc_262><loc_263><loc_275><loc_269>The</text>
<text><loc_262><loc_346><loc_331><loc_352>Cell BBox Decoder.</text>
<text><loc_262><loc_415><loc_363><loc_420>The encoding generated by the</text>
<page_footer><loc_241><loc_464><loc_245><loc_469>5</page_footer>
<page_break>
<text><loc_41><loc_48><loc_234><loc_53>tention encoding is then multiplied to the encoded image to</text>
<text><loc_51><loc_101><loc_88><loc_106>The output</text>
<text><loc_51><loc_154><loc_106><loc_160>Loss Functions.</text>
<text><loc_51><loc_283><loc_234><loc_288>The loss used to train the TableFormer can be defined as</text>
<formula><loc_103><loc_311><loc_105><loc_317></formula>
<text><loc_41><loc_336><loc_61><loc_341>where</text>
<section_header_level_1><loc_41><loc_351><loc_141><loc_358>5. Experimental Results</section_header_level_1>
<section_header_level_1><loc_41><loc_364><loc_146><loc_370>5.1. Implementation Details</section_header_level_1>
<text><loc_51><loc_376><loc_166><loc_382>TableFormer uses ResNet-18 as the</text>
<formula><loc_75><loc_413><loc_152><loc_419></formula>
<text><loc_41><loc_437><loc_234><loc_443><loc_41><loc_437><loc_234><loc_443>Although input constraints are used also by other methods, runtime performance and lower memory footprint of Table-</text>
<text><loc_262><loc_74><loc_445><loc_79>The Transformer Encoder consists of two 'Transformer</text>
<text><loc_262><loc_213><loc_445><loc_218>For training, TableFormer is trained with 3 Adam opti-</text>
<text><loc_262><loc_276><loc_445><loc_282>TableFormer is implemented with PyTorch and Torchvi-</text>
<section_header_level_1><loc_252><loc_366><loc_325><loc_372>5.2. Generalization</section_header_level_1>
<text><loc_262><loc_381><loc_445><loc_387>TableFormer is evaluated on three major publicly avail-</text>
<text><loc_262><loc_430><loc_445><loc_435>We also share our baseline results on the challenging</text>
<page_footer><loc_241><loc_464><loc_245><loc_469>6</page_footer>
<page_break>
<section_header_level_1><loc_41><loc_47><loc_137><loc_53>5.3. Datasets and Metrics</section_header_level_1>
<text><loc_51><loc_59><loc_234><loc_65>The Tree-Edit-Distance-Based Similarity (TEDS) met-</text>
<formula><loc_70><loc_99><loc_97><loc_105></formula>
<text><loc_51><loc_115><loc_71><loc_120>where</text>
<section_header_level_1><loc_41><loc_142><loc_139><loc_148>5.4. Quantitative Analysis</section_header_level_1>
<text><loc_51><loc_154><loc_86><loc_159>Structure.</text>
<otsl><loc_44><loc_258><loc_231><loc_368></otsl>
<text><loc_41><loc_374><loc_67><loc_380>Table 2:</text>
<text><loc_41><loc_389><loc_214><loc_395>FT: Model was trained on PubTabNet then finetuned.</text>
<text><loc_51><loc_407><loc_102><loc_412>Cell Detection.</text>
<text><loc_252><loc_48><loc_263><loc_53>our</text>
<otsl><loc_252><loc_156><loc_436><loc_192><caption><loc_252><loc_200><loc_279><loc_205>Table 3:</caption></otsl>
<text><loc_262><loc_232><loc_310><loc_238>Cell Content.</text>
<otsl><loc_272><loc_341><loc_426><loc_406><caption><loc_252><loc_415><loc_279><loc_420>Table 4:</caption></otsl>
<page_footer><loc_241><loc_464><loc_245><loc_469>7</page_footer>
<unordered_list><page_break>
<list_item><loc_44><loc_50><loc_408><loc_55>a. Red - PDF cells, Green - predicted bounding boxes, Blue - post-processed predictions matched to PDF cells</list_item>
</unordered_list>
<section_header_level_1><loc_44><loc_60><loc_232><loc_64>Japanese language (previously unseen by TableFormer):</section_header_level_1>
<section_header_level_1><loc_249><loc_60><loc_352><loc_64>Example table from FinTabNet:</section_header_level_1>
<picture><loc_41><loc_65><loc_246><loc_118></picture>
<picture><loc_250><loc_62><loc_453><loc_114></picture>
<caption><loc_44><loc_132><loc_315><loc_136>b. Structure predicted by TableFormer, with superimposed matched PDF cell text:</caption>
<otsl><loc_44><loc_138><loc_244><loc_185></otsl>
<otsl><loc_249><loc_138><loc_450><loc_182></otsl>
<caption><loc_311><loc_185><loc_449><loc_188>Text is aligned to match original for ease of viewing</caption>
<picture><loc_313><loc_241><loc_443><loc_280><caption><loc_41><loc_203><loc_71><loc_208>Figure 5:</caption></picture>
<picture><loc_42><loc_240><loc_173><loc_280><caption><loc_51><loc_290><loc_81><loc_295>Figure 6:</caption></picture>
<section_header_level_1><loc_41><loc_310><loc_134><loc_316>5.5. Qualitative Analysis</section_header_level_1>
<text><loc_51><loc_339><loc_97><loc_345>We showcase</text>
<picture><loc_177><loc_240><loc_307><loc_280></picture>
<section_header_level_1><loc_252><loc_310><loc_377><loc_317>6. Future Work & Conclusion</section_header_level_1>
<text><loc_262><loc_324><loc_445><loc_329>In this paper, we presented TableFormer an end-to-end</text>
<section_header_level_1><loc_252><loc_424><loc_298><loc_431>References</section_header_level_1>
<unordered_list><list_item><loc_256><loc_438><loc_265><loc_443>[1]</list_item>
</unordered_list>
<page_footer><loc_241><loc_464><loc_245><loc_469>8</page_footer>
<unordered_list><page_break>
<list_item><loc_45><loc_76><loc_53><loc_81>[2]</list_item>
<list_item><loc_45><loc_97><loc_53><loc_102>[3]</list_item>
<list_item><loc_45><loc_118><loc_53><loc_123>[4]</list_item>
<list_item><loc_45><loc_146><loc_53><loc_151>[5]</list_item>
<list_item><loc_45><loc_174><loc_53><loc_178>[6]</list_item>
<list_item><loc_45><loc_201><loc_53><loc_206>[7]</list_item>
<list_item><loc_45><loc_222><loc_53><loc_227>[8]</list_item>
<list_item><loc_45><loc_257><loc_53><loc_262>[9]</list_item>
<list_item><loc_41><loc_278><loc_53><loc_283>[10]</list_item>
<list_item><loc_41><loc_306><loc_53><loc_311>[11]</list_item>
<list_item><loc_41><loc_341><loc_53><loc_346>[12]</list_item>
<list_item><loc_41><loc_376><loc_53><loc_380>[13]</list_item>
<list_item><loc_41><loc_410><loc_53><loc_415>[14]</list_item>
<list_item><loc_41><loc_431><loc_53><loc_436>[15]</list_item>
<list_item><loc_57><loc_48><loc_174><loc_53>end object detection with transformers.</list_item>
<list_item><loc_252><loc_48><loc_265><loc_53>[16]</list_item>
<list_item><loc_252><loc_90><loc_265><loc_95>[17]</list_item>
<list_item><loc_252><loc_111><loc_265><loc_116>[18]</list_item>
<list_item><loc_252><loc_167><loc_265><loc_171>[19]</list_item>
<list_item><loc_252><loc_208><loc_265><loc_213>[20]</list_item>
<list_item><loc_252><loc_236><loc_265><loc_241>[21]</list_item>
<list_item><loc_252><loc_278><loc_265><loc_283>[22]</list_item>
<list_item><loc_252><loc_355><loc_265><loc_359>[23]</list_item>
<list_item><loc_252><loc_396><loc_265><loc_401>[24]</list_item>
<list_item><loc_252><loc_424><loc_265><loc_429>[25]</list_item>
</unordered_list>
<page_footer><loc_241><loc_464><loc_245><loc_469>9</page_footer>
<unordered_list><page_break>
<list_item><loc_41><loc_62><loc_53><loc_67>[26]</list_item>
<list_item><loc_41><loc_104><loc_53><loc_109>[27]</list_item>
<list_item><loc_41><loc_146><loc_53><loc_150>[28]</list_item>
<list_item><loc_41><loc_174><loc_53><loc_178>[29]</list_item>
<list_item><loc_41><loc_208><loc_53><loc_213>[30]</list_item>
<list_item><loc_41><loc_243><loc_53><loc_248>[31]</list_item>
<list_item><loc_41><loc_292><loc_53><loc_297>[32]</list_item>
<list_item><loc_41><loc_320><loc_53><loc_325>[33]</list_item>
<list_item><loc_41><loc_348><loc_53><loc_352>[34]</list_item>
<list_item><loc_41><loc_376><loc_53><loc_380>[35]</list_item>
<list_item><loc_41><loc_403><loc_53><loc_408>[36]</list_item>
<list_item><loc_41><loc_438><loc_53><loc_443>[37]</list_item>
</unordered_list>
<text><loc_57><loc_48><loc_183><loc_53>Computer Vision and Pattern Recognition</text>
<unordered_list><list_item><loc_252><loc_76><loc_265><loc_81>[38]</list_item>
<list_item><loc_269><loc_48><loc_313><loc_53>and evaluation.</list_item>
</unordered_list>
<page_footer><loc_239><loc_464><loc_247><loc_469>10</page_footer>
<page_break>
<section_header_level_1><loc_109><loc_70><loc_380><loc_77>TableFormer: Table Structure Understanding with Transformers</section_header_level_1>
<section_header_level_1><loc_41><loc_102><loc_144><loc_109>1. Details on the datasets</section_header_level_1>
<text><loc_252><loc_103><loc_349><loc_108>ances in regard to their size,</text>
<section_header_level_1><loc_41><loc_114><loc_123><loc_120>1.1. Data preparation</section_header_level_1>
<text><loc_51><loc_126><loc_234><loc_132>As a first step of our data preparation process, we have</text>
<text><loc_51><loc_247><loc_193><loc_253>We have developed a technique that tries</text>
<text><loc_51><loc_398><loc_234><loc_404>Figure 7 illustrates the distribution of the tables across</text>
<section_header_level_1><loc_41><loc_418><loc_125><loc_424>1.2. Synthetic datasets</section_header_level_1>
<text><loc_51><loc_430><loc_234><loc_436>Aiming to train and evaluate our models in a broader</text>
<section_header_level_1><loc_252><loc_393><loc_260><loc_400>2.</section_header_level_1>
<text><loc_262><loc_134><loc_445><loc_139>The process of generating a synthetic dataset can be de-</text>
<unordered_list><list_item><loc_262><loc_149><loc_268><loc_154>1.</list_item>
<list_item><loc_262><loc_202><loc_268><loc_208>2.</list_item>
<list_item><loc_262><loc_286><loc_397><loc_291>3. Generate content: Based on the dataset</list_item>
<list_item><loc_262><loc_317><loc_268><loc_322>4.</list_item>
<list_item><loc_262><loc_347><loc_268><loc_353>5.</list_item>
</unordered_list>
<text><loc_262><loc_415><loc_445><loc_421>Although TableFormer can predict the table structure and</text>
<page_footer><loc_239><loc_464><loc_247><loc_469>11</page_footer>
<page_break>
<picture><loc_44><loc_47><loc_445><loc_93><caption><loc_41><loc_105><loc_71><loc_110>Figure 7:</caption></picture>
<unordered_list><list_item><loc_50><loc_133><loc_53><loc_139>•</list_item>
<list_item><loc_50><loc_154><loc_53><loc_160>•</list_item>
</unordered_list>
<text><loc_51><loc_176><loc_234><loc_182>However, it is possible to mitigate those limitations by</text>
<text><loc_51><loc_252><loc_234><loc_258>Here is a step-by-step description of the prediction post-</text>
<unordered_list><list_item><loc_51><loc_267><loc_234><loc_273>1. Get the minimal grid dimensions - number of rows and</list_item>
<list_item><loc_51><loc_290><loc_57><loc_296>2.</list_item>
<list_item><loc_51><loc_321><loc_57><loc_326>3.</list_item>
<list_item><loc_51><loc_336><loc_62><loc_341>3.a.</list_item>
<list_item><loc_51><loc_359><loc_57><loc_364>4.</list_item>
</unordered_list>
<text><loc_41><loc_422><loc_61><loc_427>where</text>
<unordered_list><list_item><loc_51><loc_437><loc_57><loc_443>5.</list_item>
</unordered_list>
<text><loc_252><loc_133><loc_356><loc_139>dian cell size for all table cells.</text>
<unordered_list><list_item><loc_262><loc_164><loc_268><loc_169>6.</list_item>
<list_item><loc_262><loc_179><loc_268><loc_184>7.</list_item>
<list_item><loc_262><loc_247><loc_268><loc_253>8.</list_item>
<list_item><loc_262><loc_293><loc_268><loc_298>9.</list_item>
</unordered_list>
<text><loc_262><loc_361><loc_272><loc_366>9a.</text>
<unordered_list><list_item><loc_262><loc_384><loc_272><loc_389>9b.</list_item>
<list_item><loc_262><loc_399><loc_272><loc_404>9c.</list_item>
<list_item><loc_262><loc_422><loc_445><loc_427>9d. Intersect the orphan's bounding box with the column</list_item>
<list_item><loc_262><loc_437><loc_445><loc_443>9e. If the table cell under the identified row and column</list_item>
</unordered_list>
<formula><loc_92><loc_394><loc_129><loc_400></formula>
<page_footer><loc_239><loc_464><loc_247><loc_469>12</page_footer>
<page_break>
<text><loc_41><loc_48><loc_73><loc_53>phan cell.</text>
<text><loc_51><loc_55><loc_60><loc_61>9f.</text>
<text><loc_51><loc_70><loc_234><loc_76>Aditional images with examples of TableFormer predic-</text>
<otsl><loc_69><loc_99><loc_195><loc_135></otsl>
<otsl><loc_68><loc_148><loc_195><loc_184></otsl>
<otsl><loc_69><loc_195><loc_195><loc_232></otsl>
<otsl><loc_68><loc_250><loc_203><loc_308></otsl>
<caption><loc_52><loc_317><loc_81><loc_323>Figure 8:</caption>
<otsl><loc_254><loc_64><loc_454><loc_86><caption><loc_252><loc_194><loc_282><loc_199>Figure 9:</caption></otsl>
<otsl><loc_253><loc_98><loc_454><loc_117></otsl>
<otsl><loc_253><loc_124><loc_454><loc_147></otsl>
<picture><loc_253><loc_160><loc_348><loc_185></picture>
<otsl><loc_253><loc_160><loc_348><loc_185></otsl>
<otsl><loc_274><loc_245><loc_400><loc_276><caption><loc_255><loc_430><loc_289><loc_435>Figure 10:</caption></otsl>
<otsl><loc_274><loc_287><loc_400><loc_317></otsl>
<otsl><loc_274><loc_328><loc_401><loc_358></otsl>
<picture><loc_273><loc_374><loc_424><loc_420></picture>
<otsl><loc_273><loc_374><loc_424><loc_420></otsl>
<page_footer><loc_239><loc_464><loc_247><loc_469>13</page_footer>
<page_break>
<otsl><loc_42><loc_173><loc_231><loc_217></otsl>
<picture><loc_42><loc_66><loc_231><loc_218></picture>
<caption><loc_41><loc_225><loc_76><loc_231>Figure 11:</caption>
<otsl><loc_42><loc_286><loc_254><loc_310></otsl>
<otsl><loc_42><loc_318><loc_254><loc_342></otsl>
<otsl><loc_42><loc_350><loc_254><loc_374></otsl>
<picture><loc_41><loc_386><loc_145><loc_414></picture>
<caption><loc_45><loc_424><loc_78><loc_430>Figure 12:</caption>
<otsl><loc_261><loc_102><loc_437><loc_135></otsl>
<otsl><loc_261><loc_143><loc_437><loc_177></otsl>
<otsl><loc_268><loc_182><loc_428><loc_226></otsl>
<picture><loc_260><loc_57><loc_437><loc_227><caption><loc_258><loc_235><loc_292><loc_240>Figure 13:</caption></picture>
<otsl><loc_261><loc_272><loc_424><loc_302><caption><loc_282><loc_432><loc_316><loc_437>Figure 14:</caption></otsl>
<otsl><loc_261><loc_309><loc_424><loc_338></otsl>
<otsl><loc_261><loc_345><loc_425><loc_374></otsl>
<otsl><loc_261><loc_385><loc_436><loc_422></otsl>
<page_footer><loc_239><loc_464><loc_247><loc_469>14</page_footer>
<page_break>
<picture><loc_45><loc_86><loc_228><loc_157><caption><loc_69><loc_407><loc_103><loc_412>Figure 15:</caption></picture>
<otsl><loc_45><loc_86><loc_228><loc_157></otsl>
<picture><loc_44><loc_164><loc_228><loc_236></picture>
<otsl><loc_44><loc_164><loc_228><loc_236></otsl>
<picture><loc_45><loc_243><loc_229><loc_314></picture>
<picture><loc_41><loc_319><loc_261><loc_399></picture>
<otsl><loc_41><loc_319><loc_261><loc_399></otsl>
<otsl><loc_264><loc_77><loc_430><loc_141><caption><loc_252><loc_412><loc_286><loc_418>Figure 16:</caption></otsl>
<otsl><loc_264><loc_153><loc_430><loc_217></otsl>
<picture><loc_264><loc_229><loc_430><loc_293></picture>
<otsl><loc_264><loc_229><loc_430><loc_293></otsl>
<picture><loc_289><loc_308><loc_405><loc_401></picture>
<otsl><loc_289><loc_308><loc_405><loc_401></otsl>
<page_footer><loc_239><loc_464><loc_247><loc_469>15</page_footer>
<page_break>
<picture><loc_55><loc_160><loc_432><loc_314><caption><loc_41><loc_321><loc_75><loc_326>Figure 17:</caption></picture>
<page_footer><loc_239><loc_464><loc_247><loc_469>16</page_footer>
</doctag>