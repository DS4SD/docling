<doctag><page_header><loc_15><loc_104><loc_30><loc_350>arXiv:2206.01062v1  [cs.CV]  2 Jun 2022</page_header>
<section_header_level_1><loc_88><loc_53><loc_413><loc_63>DocLayNet: A Large Human-Annotated Dataset for</section_header_level_1>
<text><loc_83><loc_85><loc_149><loc_91>Birgit Pfitzmann</text>
<text><loc_219><loc_85><loc_282><loc_91>Christoph Auer</text>
<text><loc_358><loc_85><loc_411><loc_91>Michele Dolfi</text>
<text><loc_149><loc_122><loc_217><loc_128>Ahmed S. Nassar</text>
<section_header_level_1><loc_44><loc_157><loc_91><loc_163>ABSTRACT</section_header_level_1>
<text><loc_44><loc_166><loc_241><loc_171>Accurate document layout analysis is a key requirement for high-</text>
<section_header_level_1><loc_44><loc_348><loc_110><loc_354>CCS CONCEPTS</section_header_level_1>
<text><loc_44><loc_358><loc_47><loc_363>â€¢</text>
<text><loc_44><loc_401><loc_240><loc_405><loc_44><loc_401><loc_240><loc_405>Permission to make digital or hard copies of part or all of this work for personal or https://doi.org/10.1145/3534678.3539043</text>
<text><loc_295><loc_122><loc_339><loc_128>Peter Staar</text>
<picture><loc_264><loc_158><loc_452><loc_332><caption><loc_260><loc_341><loc_288><loc_346>Figure 1:</caption></picture>
<section_header_level_1><loc_260><loc_375><loc_310><loc_380>KEYWORDS</section_header_level_1>
<text><loc_260><loc_384><loc_457><loc_389>PDF document conversion, layout segmentation, object-detection,</text>
<section_header_level_1><loc_260><loc_405><loc_331><loc_409>ACMReference Format:</section_header_level_1>
<text><loc_260><loc_411><loc_456><loc_415>Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter</text>
<page_break>
<page_header><loc_44><loc_38><loc_201><loc_43>KDD '22, August 14-18, 2022, Washington, DC, USA</page_header>
<section_header_level_1><loc_44><loc_55><loc_49><loc_61>1</section_header_level_1>
<text><loc_44><loc_70><loc_248><loc_75>Despite the substantial improvements achieved with machine-learning</text>
<text><loc_52><loc_146><loc_241><loc_151>Akeyproblem in the process of document conversion is to under-</text>
<text><loc_52><loc_319><loc_241><loc_324>In this paper, we present the DocLayNet dataset. It provides page-</text>
<unordered_list><list_item><loc_53><loc_369><loc_61><loc_374>(1)</list_item>
<list_item><loc_53><loc_390><loc_61><loc_395>(2)</list_item>
<list_item><loc_53><loc_404><loc_61><loc_409>(3)</list_item>
<list_item><loc_53><loc_425><loc_61><loc_430>(4)</list_item>
</unordered_list>
<footnote><loc_44><loc_442><loc_46><loc_445>1</footnote>
<text><loc_279><loc_55><loc_456><loc_60>This enables experimentation with annotation uncertainty</text>
<unordered_list><list_item><loc_269><loc_69><loc_276><loc_74>(5)</list_item>
</unordered_list>
<text><loc_268><loc_106><loc_457><loc_111>All aspects outlined above are detailed in Section 3. In Section 4,</text>
<text><loc_268><loc_141><loc_456><loc_146>In Section 5, we will present baseline accuracy numbers for a</text>
<section_header_level_1><loc_260><loc_203><loc_264><loc_209>2</section_header_level_1>
<text><loc_259><loc_219><loc_457><loc_224>While early approaches in document-layout analysis used rule-</text>
<text><loc_268><loc_295><loc_456><loc_300>Lately, new types of ML models for document-layout analysis</text>
<section_header_level_1><loc_260><loc_357><loc_264><loc_363>3</section_header_level_1>
<text><loc_260><loc_373><loc_456><loc_378>DocLayNet contains 80863 PDF pages. Among these, 7059 carry two</text>
<text><loc_268><loc_428><loc_456><loc_433>In addition to open intellectual property constraints for the</text>
<page_break>
<page_header><loc_44><loc_38><loc_284><loc_43>DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis</page_header>
<page_header><loc_299><loc_38><loc_456><loc_43>KDD '22, August 14-18, 2022, Washington, DC, USA</page_header>
<footnote><loc_44><loc_442><loc_46><loc_445>2</footnote>
<picture><loc_72><loc_59><loc_215><loc_139><caption><loc_44><loc_150><loc_240><loc_154>Figure 2: Distribution of DocLayNet pages across document</caption></picture>
<text><loc_44><loc_178><loc_240><loc_183>to a minimum, since they introduce difficulties in annotation (see</text>
<text><loc_52><loc_234><loc_241><loc_238>The pages in DocLayNet can be grouped into six distinct cate-</text>
<text><loc_52><loc_323><loc_241><loc_328>We did not control the document selection with regard to lan-</text>
<text><loc_52><loc_386><loc_241><loc_391>To ensure that future benchmarks in the document-layout analy-</text>
<text><loc_268><loc_55><loc_456><loc_60>Table 1 shows the overall frequency and distribution of the labels</text>
<text><loc_268><loc_104><loc_456><loc_109>In order to accommodate the different types of models currently</text>
<text><loc_268><loc_173><loc_457><loc_178>Despite being cost-intense and far less scalable than automation,</text>
<section_header_level_1><loc_260><loc_384><loc_264><loc_389>4</section_header_level_1>
<text><loc_260><loc_399><loc_456><loc_404>The annotation campaign was carried out in four phases. In phase</text>
<page_break>
<page_header><loc_44><loc_38><loc_201><loc_43>KDD '22, August 14-18, 2022, Washington, DC, USA</page_header>
<otsl><loc_81><loc_87><loc_419><loc_186><caption><loc_44><loc_350><loc_242><loc_355>Figure 3: Corpus Conversion Service annotation user inter-</caption></otsl>
<picture><loc_43><loc_196><loc_242><loc_341><caption><loc_44><loc_54><loc_456><loc_59>Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %</caption></picture>
<text><loc_44><loc_401><loc_240><loc_406>we distributed the annotation workload and performed continuous</text>
<text><loc_52><loc_429><loc_185><loc_433>Phase 1: Data selection and preparation.</text>
<text><loc_260><loc_197><loc_393><loc_202>include publication repositories such as arXiv</text>
<text><loc_268><loc_239><loc_456><loc_244>Preparation work included uploading and parsing the sourced</text>
<text><loc_268><loc_322><loc_394><loc_327>Phase 2: Label selection and guideline.</text>
<footnote><loc_260><loc_443><loc_262><loc_446>3</footnote>
<page_break>
<page_header><loc_44><loc_38><loc_284><loc_43>DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis</page_header>
<page_header><loc_299><loc_38><loc_456><loc_43>KDD '22, August 14-18, 2022, Washington, DC, USA</page_header>
<text><loc_44><loc_55><loc_240><loc_60>the textual content of an element, which goes beyond visual layout</text>
<text><loc_52><loc_69><loc_240><loc_74>At first sight, the task of visual document-layout interpretation</text>
<text><loc_52><loc_159><loc_240><loc_164>Obviously, this inconsistency in annotations is not desirable for</text>
<unordered_list><list_item><loc_53><loc_220><loc_61><loc_225>(1)</list_item>
<list_item><loc_53><loc_248><loc_61><loc_253>(2)</list_item>
<list_item><loc_53><loc_276><loc_61><loc_281>(3)</list_item>
<list_item><loc_53><loc_289><loc_61><loc_294>(4)</list_item>
<list_item><loc_53><loc_303><loc_61><loc_308>(5)</list_item>
<list_item><loc_53><loc_310><loc_61><loc_315>(6)</list_item>
</unordered_list>
<text><loc_44><loc_337><loc_240><loc_342>The complete annotation guideline is over 100 pages long and a</text>
<text><loc_52><loc_365><loc_112><loc_369>Phase 3: Training.</text>
<picture><loc_258><loc_54><loc_457><loc_290><caption><loc_260><loc_299><loc_456><loc_304>Figure 4: Examples of plausible annotation alternatives for</caption></picture>
<text><loc_259><loc_332><loc_456><loc_337>were carried out over a timeframe of 12 weeks, after which 8 of the</text>
<text><loc_268><loc_346><loc_373><loc_351>Phase 4: Production annotation.</text>
<page_break>
<page_header><loc_44><loc_38><loc_201><loc_43>KDD '22, August 14-18, 2022, Washington, DC, USA</page_header>
<text><loc_44><loc_55><loc_240><loc_60>Table 2: Prediction performance (mAP@0.5-0.95) of object</text>
<otsl><loc_51><loc_124><loc_233><loc_222></otsl>
<text><loc_44><loc_234><loc_240><loc_239>to avoid this at any cost in order to have clear, unbiased baseline</text>
<section_header_level_1><loc_44><loc_372><loc_49><loc_378>5</section_header_level_1>
<text><loc_44><loc_387><loc_240><loc_392>The primary goal of DocLayNet is to obtain high-quality ML models</text>
<picture><loc_264><loc_57><loc_452><loc_164><caption><loc_260><loc_177><loc_456><loc_181>Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask</caption></picture>
<text><loc_260><loc_243><loc_456><loc_248>paper and leave the detailed evaluation of more recent methods</text>
<text><loc_268><loc_257><loc_456><loc_261>In this section, we will present several aspects related to the</text>
<section_header_level_1><loc_260><loc_314><loc_381><loc_320>Baselines for Object Detection</section_header_level_1>
<text><loc_260><loc_324><loc_456><loc_328>In Table 2, we present baseline experiments (given in mAP) on Mask</text>
<page_break>
<page_header><loc_44><loc_38><loc_284><loc_43>DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis</page_header>
<page_header><loc_299><loc_38><loc_456><loc_43>KDD '22, August 14-18, 2022, Washington, DC, USA</page_header>
<text><loc_44><loc_55><loc_240><loc_60>Table 3: Performance of a Mask R-CNN R50 network in</text>
<otsl><loc_66><loc_95><loc_218><loc_187></otsl>
<section_header_level_1><loc_44><loc_202><loc_107><loc_208>Learning Curve</section_header_level_1>
<text><loc_44><loc_212><loc_240><loc_217>One of the fundamental questions related to any dataset is if it is</text>
<section_header_level_1><loc_44><loc_343><loc_134><loc_349>Impact of Class Labels</section_header_level_1>
<text><loc_44><loc_352><loc_240><loc_357>The choice and number of labels can have a significant effect on</text>
<text><loc_260><loc_55><loc_456><loc_60>Table 4: Performance of a Mask R-CNN R50 network with</text>
<otsl><loc_288><loc_95><loc_427><loc_193></otsl>
<text><loc_260><loc_209><loc_456><loc_214>lists in PubLayNet (grouped list-items) versus DocLayNet (separate</text>
<section_header_level_1><loc_260><loc_272><loc_449><loc_277>Impact of Document Split in Train and Test Set</section_header_level_1>
<text><loc_260><loc_281><loc_456><loc_286>Many documents in DocLayNet have a unique styling. In order</text>
<section_header_level_1><loc_260><loc_385><loc_342><loc_390>Dataset Comparison</section_header_level_1>
<text><loc_260><loc_394><loc_456><loc_399>Throughout this paper, we claim that DocLayNet's wider variety of</text>
<page_break>
<page_header><loc_44><loc_38><loc_201><loc_43>KDD '22, August 14-18, 2022, Washington, DC, USA</page_header>
<text><loc_44><loc_55><loc_240><loc_60>Table 5: Prediction Performance (mAP@0.5-0.95) of a Mask</text>
<otsl><loc_59><loc_109><loc_225><loc_215></otsl>
<text><loc_44><loc_247><loc_87><loc_252>Section-header</text>
<text><loc_52><loc_282><loc_240><loc_287>For comparison of DocBank with DocLayNet, we trained only</text>
<section_header_level_1><loc_44><loc_383><loc_127><loc_388>Example Predictions</section_header_level_1>
<text><loc_44><loc_392><loc_241><loc_397>To conclude this section, we illustrate the quality of layout predic-</text>
<section_header_level_1><loc_260><loc_55><loc_264><loc_61>6</section_header_level_1>
<text><loc_260><loc_64><loc_456><loc_69>In this paper, we presented the DocLayNet dataset. It provides the</text>
<text><loc_268><loc_119><loc_456><loc_124>From the dataset, we have derived on the one hand reference</text>
<text><loc_268><loc_182><loc_456><loc_187>To date, there is still a significant gap between human and ML</text>
<section_header_level_1><loc_260><loc_212><loc_316><loc_218>REFERENCES</section_header_level_1>
<unordered_list><list_item><loc_262><loc_220><loc_269><loc_224>[1]</list_item>
<list_item><loc_262><loc_235><loc_269><loc_239>[2]</list_item>
<list_item><loc_262><loc_256><loc_269><loc_259>[3]</list_item>
<list_item><loc_262><loc_271><loc_269><loc_274>[4]</list_item>
<list_item><loc_262><loc_291><loc_269><loc_295>[5]</list_item>
<list_item><loc_262><loc_311><loc_269><loc_315>[6]</list_item>
<list_item><loc_262><loc_326><loc_269><loc_330>[7]</list_item>
<list_item><loc_262><loc_351><loc_269><loc_355>[8]</list_item>
<list_item><loc_262><loc_366><loc_269><loc_370>[9]</list_item>
<list_item><loc_260><loc_386><loc_269><loc_390>[10]</list_item>
<list_item><loc_260><loc_396><loc_269><loc_400>[11]</list_item>
<list_item><loc_260><loc_412><loc_269><loc_415>[12]</list_item>
<list_item><loc_260><loc_427><loc_269><loc_430>[13]</list_item>
</unordered_list>
<page_break>
<page_header><loc_44><loc_38><loc_284><loc_43>DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis</page_header>
<page_header><loc_299><loc_38><loc_456><loc_43>KDD '22, August 14-18, 2022, Washington, DC, USA</page_header>
<picture><loc_43><loc_53><loc_455><loc_279><caption><loc_51><loc_280><loc_58><loc_283>Text</caption></picture>
<text><loc_44><loc_294><loc_456><loc_298>Figure 6: Example layout predictions on selected pages from the DocLayNet test-set. (A, D) exhibit favourable results on</text>
<text><loc_57><loc_333><loc_241><loc_337>Diaconu, Mai Thanh Minh, Marc, albinxavi, fatih, oleg, and wanghao yang. ul-</text>
<unordered_list><list_item><loc_260><loc_333><loc_269><loc_337>[20]</list_item>
<list_item><loc_260><loc_343><loc_269><loc_347>[21]</list_item>
<list_item><loc_260><loc_358><loc_269><loc_362>[22]</list_item>
<list_item><loc_260><loc_378><loc_269><loc_382>[23]</list_item>
<list_item><loc_44><loc_348><loc_53><loc_352>[14]</list_item>
<list_item><loc_44><loc_363><loc_53><loc_367>[15]</list_item>
<list_item><loc_44><loc_373><loc_53><loc_377>[16]</list_item>
<list_item><loc_44><loc_388><loc_53><loc_392>[17]</list_item>
<list_item><loc_44><loc_398><loc_53><loc_402>[18]</list_item>
<list_item><loc_44><loc_424><loc_53><loc_427>[19]</list_item>
</unordered_list>
</doctag>